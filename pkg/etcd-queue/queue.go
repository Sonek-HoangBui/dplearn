// Package etcdqueue implements queue service backed by etcd.
package etcdqueue

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"net/url"
	"path"
	"sync"
	"time"

	"github.com/coreos/etcd/clientv3"
	"github.com/coreos/etcd/embed"
	"github.com/coreos/etcd/etcdserver/api/v3client"
	"github.com/coreos/etcd/mvcc/mvccpb"
	"github.com/golang/glog"
)

// Item is a job item.
// Key is used as a key in etcd.
// Marshalled JSON struct data as a value.
type Item struct {
	// Bucket is the name or job category for namespacing.
	// All keys will be prefixed with this bucket name.
	Bucket string `json:"bucket"`

	// CreatedAt is the time when this item is created.
	CreatedAt time.Time `json:"created_at"`

	// Key is autogenerated and used as a key when written to etcd.
	Key string `json:"key"`

	// Value contains any data (e.g. encoded computation results).
	Value string `json:"value"`

	// Progress is the progress status value (range from 0 to 100).
	Progress int `json:"progress"`

	// Canceled is true.
	Canceled bool `json:"canceled"`

	// Error contains any error message.
	// It's defined as string for different language interpolation.
	Error string `json:"error"`
}

// CreateItem creates an item with auto-generated ID. The ID uses unix
// nano seconds, so that items created later are added in order.
// The maximum weight(priority) is 99999.
func CreateItem(bucket string, weight uint64, value string) *Item {
	if weight > 99999 {
		weight = 99999
	}
	return &Item{
		Bucket:    bucket,
		CreatedAt: time.Now(),
		Key:       path.Join(pfxScheduled, bucket, fmt.Sprintf("%05d%035X", weight, time.Now().UnixNano())),
		Value:     value,
		Progress:  0,
		Error:     "",
	}
}

// Queue is the queue service backed by etcd.
type Queue interface {
	// ClientEndpoints returns the client endpoints.
	ClientEndpoints() []string

	// Client returns the client.
	Client() *clientv3.Client

	// Stop stops the queue and its clients.
	Stop()

	// Add adds an item to the queue.
	// Updates are sent to the returned channel.
	// And the channel is closed after key deletion,
	// which is the last event when the job is completed.
	Add(ctx context.Context, it *Item) (ItemWatcher, error)

	// Delete deletes the item from the queue.
	// The item is dequeue-ed from the queue, and canceled if in progress.
	Delete(ctx context.Context, it *Item) error
}

// ItemWatcher is for clients that subscribes the status of job item.
type ItemWatcher <-chan *Item

const (
	pfxWorker    = "queue_worker"    // ready/in-progress in worker process
	pfxScheduled = "queue_scheduled" // requested by client, added on queue
	pfxCompleted = "queue_completed" // finished by worker

	// progress value 100 means that the job is done!
	maxProgress = 100
)

type queue struct {
	mu         sync.RWMutex
	cli        *clientv3.Client
	rootCtx    context.Context
	rootCancel func()
	buckets    map[string]chan error
}

// embeddedQueue implements Queue interface with a single-node embedded etcd cluster.
type embeddedQueue struct {
	srv *embed.Etcd
	Queue
}

// NewEmbeddedQueue starts a new embedded etcd server.
// cport is the TCP port used for etcd client request serving.
// pport is for etcd peer traffic, and still needed even if it's a single-node cluster.
func NewEmbeddedQueue(ctx context.Context, cport, pport int, dataDir string) (Queue, error) {
	cfg := embed.NewConfig()
	cfg.ClusterState = embed.ClusterStateFlagNew

	cfg.Name = "etcd-queue"
	cfg.Dir = dataDir

	curl := url.URL{Scheme: "http", Host: fmt.Sprintf("localhost:%d", cport)}
	cfg.ACUrls, cfg.LCUrls = []url.URL{curl}, []url.URL{curl}

	purl := url.URL{Scheme: "http", Host: fmt.Sprintf("localhost:%d", pport)}
	cfg.APUrls, cfg.LPUrls = []url.URL{purl}, []url.URL{purl}

	cfg.InitialCluster = fmt.Sprintf("%s=%s", cfg.Name, cfg.APUrls[0].String())

	// auto-compaction every hour
	cfg.AutoCompactionRetention = 1
	// single-node, so aggressively snapshot/discard Raft log entries
	cfg.SnapCount = 1000

	glog.Infof("starting %q with endpoint %q", cfg.Name, curl.String())
	srv, err := embed.StartEtcd(cfg)
	if err != nil {
		return nil, err
	}
	select {
	case <-srv.Server.ReadyNotify():
		err = nil
	case err = <-srv.Err():
	case <-srv.Server.StopNotify():
		err = fmt.Errorf("received from etcdserver.Server.StopNotify")
	case <-ctx.Done():
		err = ctx.Err()
	}
	if err != nil {
		return nil, err
	}
	glog.Infof("started %q with endpoint %q", cfg.Name, curl.String())

	cli := v3client.New(srv.Server)

	// issue linearized read to ensure leader election
	glog.Infof("sending GET to endpoint %q", curl.String())
	_, err = cli.Get(ctx, "foo")
	glog.Infof("sent GET to endpoint %q (error: %v)", curl.String(), err)

	cctx, cancel := context.WithCancel(ctx)
	return &embeddedQueue{
		srv: srv,
		Queue: &queue{
			cli:        cli,
			rootCtx:    cctx,
			rootCancel: cancel,
			buckets:    make(map[string]chan error),
		},
	}, err
}

func (qu *embeddedQueue) ClientEndpoints() []string {
	eps := make([]string, len(qu.srv.Config().LCUrls))
	for i := range qu.srv.Config().LCUrls {
		eps = append(eps, qu.srv.Config().LCUrls[i].String())
	}
	return eps
}

func (qu *embeddedQueue) Stop() {
	glog.Info("stopping queue with an embedded etcd server")
	qu.Queue.Stop()
	qu.srv.Close()
	glog.Info("stopped queue with an embedded etcd server")
}

// NewQueue creates a new queue from given etcd client.
func NewQueue(cli *clientv3.Client) (Queue, error) {
	// issue linearized read to ensure leader election
	glog.Infof("GET request to endpoint %v", cli.Endpoints())
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	_, err := cli.Get(ctx, "foo")
	cancel()
	glog.Infof("GET request succeeded on endpoint %v", cli.Endpoints())
	if err != nil {
		return nil, err
	}

	ctx, cancel = context.WithCancel(context.Background())
	return &queue{
		cli:        cli,
		rootCtx:    ctx,
		rootCancel: cancel,
		buckets:    make(map[string]chan error),
	}, nil
}

func (qu *queue) ClientEndpoints() []string { return qu.cli.Endpoints() }
func (qu *queue) Client() *clientv3.Client  { return qu.cli }
func (qu *queue) Stop() {
	qu.mu.Lock()
	defer qu.mu.Unlock()

	glog.Info("stopping queue")
	qu.rootCancel()
	for bucket, errc := range qu.buckets {
		glog.Infof("stopping bucket %q", bucket)
		if err := <-errc; err != nil && err != context.Canceled {
			glog.Warningf("watch error: %v", err)
		}
		glog.Infof("stopped bucket %q", bucket)
	}
	qu.cli.Close()
	glog.Info("stopped queue")
}

const canceledWorkerValue = `{"canceled":true}`

func (qu *queue) Delete(ctx context.Context, it *Item) error {
	// current worker key slot to watch
	workerKey := path.Join(pfxWorker, it.Bucket)

	// key is already prefixed with 'path.Join(pfxScheduled, bucket, ...'
	key := it.Key
	v, err := json.Marshal(it)
	if err != nil {
		return err
	}
	val := string(v)

	qu.mu.Lock()
	defer qu.mu.Unlock()

	glog.Infof("deleting(or canceling) the item %q", key)
	if err := qu.delete(ctx, key); err != nil {
		return err
	}
	glog.Infof("deleted(or canceled) the item %q", key)

	glog.Infof("deleting %q from worker, if in progress", key)
	var tresp *clientv3.TxnResponse
	tresp, err = qu.cli.Txn(ctx).
		If(clientv3.Compare(clientv3.Value(workerKey), "=", val)).
		Then(clientv3.OpPut(workerKey, canceledWorkerValue)).
		Commit()
	if err != nil {
		return err
	}
	if tresp.Succeeded {
		glog.Infof("deleting %q from worker (was in progress)", key)
	} else {
		glog.Infof("did not delete %q from worker (was not in progress)", key)
	}
	return nil
}

func (qu *queue) Add(ctx context.Context, it *Item) (ItemWatcher, error) {
	key := it.Key
	data, err := json.Marshal(it)
	if err != nil {
		return nil, err
	}
	val := string(data)

	qu.mu.Lock()
	defer qu.mu.Unlock()

	err = qu.put(ctx, key, val)
	if err != nil {
		return nil, err
	}

	if _, ok := qu.buckets[it.Bucket]; !ok { // first job in the bucket, so schedule right away
		if err = qu.put(ctx, path.Join(pfxWorker, it.Bucket), val); err != nil {
			return nil, err
		}
		qu.buckets[it.Bucket] = make(chan error, 1)
		go qu.runScheduler(qu.rootCtx, it.Bucket, qu.buckets[it.Bucket])
	}

	wch := qu.cli.Watch(ctx, key, clientv3.WithPrevKV())

	// TODO: configurable?
	ch := make(chan *Item, 100)

	item := *it

	// watch until it's done, close on delete/error event at the end
	go func() {
		for {
			select {
			case wresp := <-wch:
				if len(wresp.Events) != 1 {
					item.Error = fmt.Sprintf("watcher: %q expects 1 event from watch, got %+v", key, wresp.Events)
					ch <- &item
					close(ch)
					return
				}
				if wresp.Events[0].Type == mvccpb.DELETE {
					glog.Infof("watcher: %q has been deleted; either completed or canceled", key)
					if wresp.Events[0].PrevKv != nil {
						item.Value = string(wresp.Events[0].PrevKv.Value)
					}
					var oldItem Item
					if err := json.Unmarshal(wresp.Events[0].PrevKv.Value, &oldItem); err != nil {
						item.Error = fmt.Sprintf("watcher: cannot parse %s", item.Value)
						ch <- &item
						close(ch)
						return
					}
					if oldItem.Progress != 100 {
						item.Canceled = true
						glog.Infof("watcher: found %q progress is only %d (canceled)", key, oldItem.Progress)
					}
					ch <- &item
					close(ch)
					return
				}
				if err := json.Unmarshal(wresp.Events[0].Kv.Value, &item); err != nil {
					item.Error = fmt.Sprintf("watcher: cannot parse %s", string(wresp.Events[0].Kv.Value))
					ch <- &item
					close(ch)
					return
				}

				ch <- &item
				if item.Error != "" {
					glog.Warningf("watcher: watched item contains error %v", item.Error)
					close(ch)
					return
				}
				glog.Infof("watcher: %q has been updated", key)

			case <-ctx.Done():
				item.Error = ctx.Err().Error()
				ch <- &item
				close(ch)
				return
			}
		}
	}()
	return ch, nil
}

// run watches on the queue and schedules the jobs.
// Point is never miss events, thus one routine must always watch path.Join(pfxWorker, bucket)
// 1. blocks until worker job is done, notified via watch events
// 2. notify the client back with the new results on the key (Key field in Item)
// 3. delete the DONE key from the queue, and move to pfxCompleted + Key for logging
// 4. fetch one new job from path.Join(pfxScheduled, bucket)
// 5. skip if there is no job to schedule
// 6. write this job to path.Join(pfxWorker, bucket)
// 7. drain watch events for this write
// repeat!
func (qu *queue) run(ctx context.Context, bucket string) error {
	// slashes in http scheme keys will be munged, but ok since it's internal
	workerKey := path.Join(pfxWorker, bucket)       // current worker key slot to watch
	scheduledKey := path.Join(pfxScheduled, bucket) // current queue scheduler key to fetch from

	wch := qu.cli.Watch(ctx, workerKey)
	glog.Infof("scheduler: watching %q", workerKey)

	for {
		// 1. blocks until worker job is done, notified via watch events
		select {
		case wresp := <-wch:
			if len(wresp.Events) != 1 {
				return fmt.Errorf("scheduler: no watch events on %q (%+v, %v)", workerKey, wresp, wresp.Err())
			}
			v := wresp.Events[0].Kv.Value
			var item Item
			if err := json.Unmarshal([]byte(v), &item); err != nil {
				return fmt.Errorf("scheduler: %q returned wrong JSON value %q (%v)", workerKey, string(v), err)
			}
			if item.Progress < maxProgress {
				glog.Infof("scheduler: %q is in progress %d / %d (continue)", item.Key, item.Progress, maxProgress)
				continue
			}

			// 2. notify the client back with the new results on the key (ID field in Item)
			glog.Infof("scheduler: %q is done", item.Key)
			if err := qu.put(ctx, item.Key, string(v)); err != nil {
				return err
			}

			// 3. delete the DONE key from the queue, and move to pfxCompleted + Key for logging
			glog.Infof("scheduler: %q is deleted", item.Key)
			if err := qu.delete(ctx, item.Key); err != nil {
				return err
			}
			cKey := path.Join(pfxCompleted, item.Key)
			if err := qu.put(ctx, cKey, string(v)); err != nil {
				return err
			}
			glog.Infof("scheduler: %q is written", cKey)

			// 4. fetch one new job from path.Join(pfxScheduled, bucket)
			resp, err := qu.cli.Get(ctx, scheduledKey, append(clientv3.WithFirstKey(), clientv3.WithPrefix())...)
			if err != nil {
				return err
			}

			// 5. skip if there is no job to schedule
			if len(resp.Kvs) == 0 {
				glog.Infof("scheduler: no job to schedule on the bucket %q", bucket)
				continue
			}
			if len(resp.Kvs) != 1 {
				return fmt.Errorf("scheduler: %q should return only one key-value pair (got %+v)", scheduledKey, resp.Kvs)
			}
			fetched := resp.Kvs[0].Value
			var newItem Item
			if err := json.Unmarshal([]byte(fetched), &newItem); err != nil {
				return fmt.Errorf("scheduler: %q has wrong JSON %q", resp.Kvs[0].Key, string(fetched))
			}
			if newItem.Progress != 0 {
				return fmt.Errorf("scheduler: %q must have initial progress, got %d", newItem.Key, newItem.Progress)
			}

			// 6. write this job to path.Join(pfxWorker, bucket)
			glog.Infof("scheduler: %q is scheduled", string(resp.Kvs[0].Key))
			qu.mu.Lock() // protect against racey job cancel
			if err := qu.put(ctx, workerKey, string(resp.Kvs[0].Value)); err != nil {
				qu.mu.Unlock()
				return err
			}

			// 7. drain watch events for this write
			select {
			case wresp = <-wch:
				qu.mu.Unlock()

				if len(wresp.Events) != 1 {
					return fmt.Errorf("scheduler: no watch events on %q after schedule (%+v, %v)", workerKey, wresp, wresp.Err())
				}
				v := wresp.Events[0].Kv.Value

				if string(v) == canceledWorkerValue {
					panic("racey worker cancel")
				}

				var item Item
				if err := json.Unmarshal([]byte(v), &item); err != nil {
					return fmt.Errorf("scheduler: %q returned wrong JSON value %q (%v)", workerKey, string(v), err)
				}
				if item.Progress != 0 {
					return fmt.Errorf("scheduler: %q has wrong progress %d, expected initial progress 0", workerKey, item.Progress)
				}
				if !bytes.Equal(resp.Kvs[0].Value, v) {
					return fmt.Errorf("scheduler: scheduled value expected %q, got %q", string(resp.Kvs[0].Value), string(v))
				}

			case <-ctx.Done():
				qu.mu.Unlock()
				return ctx.Err()
			}

		case <-ctx.Done():
			return ctx.Err()
		}
	}
}

func (qu *queue) runScheduler(ctx context.Context, bucket string, errc chan error) {
	defer close(errc)

	for {
		if err := qu.run(ctx, bucket); err != nil {
			if err == context.Canceled {
				errc <- err
				return
			}
			glog.Warningf("scheduler: %v", err)
		}
		// below is implemented for failure tolerance; retry logic

		// slashes in http scheme keys will be munged, but ok since it's internal
		workerKey := path.Join(pfxWorker, bucket)       // current worker key slot to watch
		scheduledKey := path.Join(pfxScheduled, bucket) // current queue scheduler key to fetch from

		// resetting current worker job
		resp, err := qu.cli.Get(ctx, workerKey)
		if err != nil {
			errc <- err
			return
		}
		if len(resp.Kvs) != 1 {
			errc <- fmt.Errorf("scheduler: len(resp.Kvs) expected 1, got %+v", resp.Kvs)
			return
		}
		v := resp.Kvs[0].Value
		var item Item
		if err = json.Unmarshal([]byte(v), &item); err != nil {
			errc <- err
			return
		}
		if item.Progress == maxProgress {
			glog.Warningf("scheduler: watch might have failed after %q is finished", item.Key)

			// 2. notify the client back with the new results on the key (ID field in Item)
			glog.Infof("scheduler: %q is done", item.Key)
			if err := qu.put(ctx, item.Key, string(v)); err != nil {
				errc <- err
				return
			}

			// 3. delete the DONE key from the queue, and move to pfxCompleted + Key for logging
			glog.Infof("scheduler: %q is deleted", item.Key)
			if err := qu.delete(ctx, item.Key); err != nil {
				errc <- err
				return
			}
			cKey := path.Join(pfxCompleted, item.Key)
			if err := qu.put(ctx, cKey, string(v)); err != nil {
				errc <- err
				return
			}
			glog.Infof("scheduler: %q is written", cKey)

			// 4. fetch one new job from path.Join(pfxScheduled, bucket)
			resp, err := qu.cli.Get(ctx, scheduledKey, append(clientv3.WithFirstKey(), clientv3.WithPrefix())...)
			if err != nil {
				errc <- err
				return
			}

			// 5. skip if there is no job to schedule
			if len(resp.Kvs) == 0 {
				glog.Infof("scheduler: no job to schedule on the bucket %q", bucket)
				continue
			}
			if len(resp.Kvs) != 1 {
				errc <- fmt.Errorf("scheduler: %q should return only one key-value pair (got %+v)", scheduledKey, resp.Kvs)
				return
			}
			fetched := resp.Kvs[0].Value
			var newItem Item
			if err := json.Unmarshal([]byte(fetched), &newItem); err != nil {
				errc <- fmt.Errorf("scheduler: %q has wrong JSON %q", resp.Kvs[0].Key, string(fetched))
				return
			}
			if newItem.Progress != 0 {
				errc <- fmt.Errorf("scheduler: %q must have initial progress, got %d", newItem.Key, newItem.Progress)
				return
			}

			// 6. write this job to path.Join(pfxWorker, bucket)
			glog.Infof("scheduler: %q is scheduled", string(resp.Kvs[0].Key))
			if err := qu.put(ctx, workerKey, string(resp.Kvs[0].Value)); err != nil {
				errc <- err
				return
			}

			// continue to 'run'
		}
	}
}

func (qu *queue) put(ctx context.Context, key, val string) error {
	_, err := qu.cli.Put(ctx, key, val)
	return err
}

func (qu *queue) delete(ctx context.Context, key string) error {
	_, err := qu.cli.Delete(ctx, key)
	return err
}
