{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px;\"><b>Stats216v: Statistical Learning</b></div>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center\">Stanford University</div>\n",
    "<div style=\"text-align: center\">Summer 2017</div>\n",
    "<div style=\"text-align: center\">Gyu-Ho Lee (<a href=\"mailto:gyuhox@gmail.com\">gyuhox@gmail.com</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### The Supervised Learning Problem\n",
    "\n",
    "- Outcome measurement $Y$ (also called dependent variable, response, target).\n",
    "- Vector of $p$ predictor measurements $X$ (also called inputs, regressors, covariates, features, independent variables).\n",
    "- In the regression problem, $Y$ is quantitative (e.g price, blood pressure).\n",
    "- In the classification problem, $Y$ takes values in a finite, unordered set (survived/died, digit 0-9, cancer class of tissue sample).\n",
    "- We have training data $(x_{1}, y_{1}), . . . ,(x_{N}, y_{N})$. These are observations (examples, instances) of these measurements.\n",
    "\n",
    "On the basis of the training data we would like to:\n",
    "\n",
    "- Accurately predict unseen test cases.\n",
    "- Understand which inputs affect the outcome, and how.\n",
    "- Assess the quality of our predictions and inferences.\n",
    "\n",
    "\n",
    "##### Unsupervised learning\n",
    "\n",
    "- No outcome variable, just a set of predictors (features) measured on a set of samples.\n",
    "- objective is more fuzzy\n",
    "  - find groups of samples that behave similarly\n",
    "  - find features that behave similarly\n",
    "  - find linear combinations of features with the most variation.\n",
    "- difficult to know how well your are doing.\n",
    "- different from supervised learning, but can be useful as a pre-processing step for supervised learning.\n",
    "\n",
    "##### Statistical Learning versus Machine Learning\n",
    "\n",
    "- Machine learning arose as a subfield of Artificial Intelligence.\n",
    "- Statistical learning arose as a subfield of Statistics.\n",
    "- There is much overlap — both fields focus on supervised and unsupervised problems:\n",
    "- Machine learning has a greater emphasis on large scale applications and prediction accuracy.\n",
    "- Statistical learning emphasizes models and their interpretability, and precision and uncertainty.\n",
    "- But the distinction has become more and more blurred, and there is a great deal of “cross-fertilization”.\n",
    "- Machine learning has the upper hand in Marketing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.R1\n",
    "\n",
    "Which of the following are supervised learning problems? More than one box can be checked.\n",
    "\n",
    "1. Predict whether a website user will click on an ad.\n",
    "2. Find clusters of genes that interact with each other.\n",
    "3. Classify a handwritten digit as 0-9 from labeled examples.\n",
    "4. Find stocks that are likely to rise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer:\n",
    "1, 3, 4.\n",
    "</span>\n",
    "\n",
    "Problems with clearly defined \"predictors\" and \"responses\" are supervised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.R2\n",
    "\n",
    "True or False: The only goal of any supervised learning study is to be able to predict the response very accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer:\n",
    "False. Not just for \"prediction\", also for \"inference\".\n",
    "</span>\n",
    "\n",
    "False. Most supervised learning problems can be framed formally in terms of predicting a response, but prediction alone is often not the main goal of the analysis. For example, many applications of linear regression in the sciences are aimed primarily at understanding how the inputs of a system drive the outputs; an extremely complicated \"black box\" giving pure predictions would not be very useful in and of itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 2.1 Introduction to Regression Models\n",
    "\n",
    "Now we write our model as $Y = f(X) + ɛ$. With a good $f$ we can make predictions of $Y$ at new points $X = x$.\n",
    "\n",
    "$f(4) = E(Y |X = 4)$ means expected value (average) of $Y$ given $X = 4$. This ideal $f(x) = E(Y |X = x)$ is called the regression function.\n",
    "\n",
    "The ideal or optimal predictor of $Y$ with regard to mean-squared prediction error: $f(x) = E(Y |X = x)$ is the function that minimizes $E[(Y − g(X))^2|X = x]$ over all functions $g$ at all points $X = x$.\n",
    "\n",
    "$ɛ = Y − f(x)$ is the **irreducible** error — i.e. even if we knew $f(x)$, we would still make errors in prediction, since at each $X = x$ there is typically a distribution of possible $Y$ values.\n",
    "\n",
    "For any estimate $\\hat{f}(x)$ of $f(x)$, we have\n",
    "\n",
    "$$E[(Y − \\hat{f}(X))^2|X = x] = [f(x) − \\hat{f}(x)]^2 + Var(ɛ)$$.\n",
    "\n",
    "Typically we have few if any data points with $X = 4$ exactly. So we cannot compute $E(Y |X = x)$.\n",
    "\n",
    "Relax the definition and let\n",
    "\n",
    "$$f(x) = Ave(Y |X ∈ N (x))$$\n",
    "\n",
    "where $N(x)$ is some neighborhood of $x$.\n",
    "\n",
    "Nearest neighbor methods can be lousy when $p$ is large. Reason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 2.1.R1\n",
    "\n",
    "In the expression $Sales ≈ f(TV, Radio, Newspaper)$, $Sales$ is the:\n",
    "\n",
    "1. Response\n",
    "2. Training Data\n",
    "3. Independent Variable\n",
    "4. Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Response.\n",
    "</span>\n",
    "\n",
    "The variable which you are trying to model is called the response or outcome. The other variables are called features, predictors, or independent variables. Together, the collection of features and response values that you will use for fitting form your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 2.2.R1\n",
    "\n",
    "A hypercube with side length 1 in d dimensions is defined to be the set of points $(x_{1}, x_{2}, ..., x_{d})$ such that $0 ≤ x_{j} ≤ 1$ for all $j = 1, 2, ..., d$. The boundary of the hypercube is defined to be the set of all points such that there exists a $j$ for which $0 ≤ x_{j} ≤ .05$ or $.95 ≤ x_{j} ≤ 1$ (namely, the boundary is the set of all points that have at least one dimension in the most extreme $10%$ of possible values). What proportion of the points in a hypercube of dimension 50 are in the boundary? (hint: you may want to calculate the volume of the non-boundary region)\n",
    "\n",
    "Please give your answer as a value between 0 and 1 with 3 significant digits. If you think the answer is $50.52%$, you should say $0.505$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Gyu-Ho's Answer: 0.995</span>\n",
    "- <span style=\"color:blue\">The volume of hypercube with 50-dimension and side length 1 is $1^{50} = 1$.</span>\n",
    "- <span style=\"color:blue\">The volume of hypercube interior is $0.9^{50} = 0.005$.</span>\n",
    "- <span style=\"color:blue\">Thus, the volume of boundary is $1 - 0.005 = 0.995$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 2.3.R1\n",
    "\n",
    "True or False: A fitted model with more predictors will necessarily have a lower Training Set Error than a model with fewer predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer:\n",
    "False.\n",
    "</span>\n",
    "\n",
    "False. While we typically expect a model with more predictors to have lower Training Set Error, it is not necessarily the case. An extreme counterexample would be a case where you have a model with one predictor that is always equal to the response, compared to a model with many predictors that are random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.R2\n",
    "\n",
    "While doing a homework assignment, you fit a Linear Model to your data set. You are thinking about changing the Linear Model to a Quadratic one. Which of the following is most likely true:\n",
    "\n",
    "1. Using the Quadratic Model will decrease your Irreducible Error.\n",
    "2. Using the Quadratic Model will decrease the Bias of your model.\n",
    "3. Using the Quadratic Model will decrease the Variance of your model.\n",
    "4. Using the Quadratic Model will decrease your Reducible Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Using the Quadratic Model will decrease the Bias of your model, because it's more flexible.\n",
    "</span>\n",
    "\n",
    "Introducing the quadratic term will make your model more complicated. More complicated models typically have lower bias at the cost of higher variance. This has an unclear effect on Reducible Error (could go up or down) and no effect on Irreducible Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.R1\n",
    "\n",
    "Look at the graph given on page 30 of the Chapter 2 lecture slides. Which of the following is most likely true of what would happen to the Test Error curve as we move $1/K$ further above 1?\n",
    "\n",
    "<img src=\"./chapter-02-statistical-learning-errors.png\" alt=\"chapter-02-statistical-learning-errors\" style=\"width: 350px;\"/>\n",
    "\n",
    "1. The Test Errors will increase.\n",
    "2. The Test Errors will decrease.\n",
    "3. Not enough information is given to decide.\n",
    "4. It does not make sense to have $1/K > 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: It does not make sense to have $1/K > 1$.\n",
    "</span>\n",
    "\n",
    "Since $K$ is the number of neighbors, the value of $K$ must be a Natural Number. This means that $1/K ≤ 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.Q\n",
    "\n",
    "For each of the following parts, indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible model.\n",
    "\n",
    "- Flexible is better.\n",
    "- Flexible is worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.Q.1\n",
    "\n",
    "The sample size $n$ is extremely large, and the number of predictors $p$ is small:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Flexible is better, when we have much data.\n",
    "</span>\n",
    "\n",
    "A flexible model will allow us to take full advantage of our large sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.Q.2\n",
    "\n",
    "The number of predictors $p$ is extremely large, and the sample size $n$ is small:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Flexible is worse, because complex functions can be overfitting the data.\n",
    "</span>\n",
    "\n",
    "The flexible model will cause overfitting due to our small sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.Q.3\n",
    "\n",
    "The relationship between the predictors and response is highly non-linear:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Flexible is better, because simple linear regression would have high bias.\n",
    "</span>\n",
    "\n",
    "A flexible model will be necessary to find the nonlinear effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.Q.4\n",
    "\n",
    "The variance of the error terms, i.e. $σ^{2} = Var(ϵ)$, is extremely high:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Flexible is worse, because flexible methods would be fitting the irreducible error terms.\n",
    "</span>\n",
    "\n",
    "A flexible model will cause us to fit too much of the noise in the problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
