{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px;\"><b>Stats216v: Statistical Learning</b></div>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center\">Stanford University</div>\n",
    "<div style=\"text-align: center\">Summer 2017</div>\n",
    "<div style=\"text-align: center\">Gyu-Ho Lee (<a href=\"mailto:gyuhox@gmail.com\">gyuhox@gmail.com</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.R1\n",
    "\n",
    "Which of the following modeling techniques performs Feature Selection?\n",
    "\n",
    "1. Linear Discriminant Analysis\n",
    "2. Least Squares\n",
    "3. Linear Regression with Forward Selection\n",
    "4. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 3.\n",
    "</span>\n",
    "\n",
    "Forward Selection chooses a subset of the predictor variables for the final model. The other three methods end up using all of the predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.R1\n",
    "\n",
    "We perform best subset and forward stepwise selection on a single dataset. For both approaches, we obtain $p + 1$ models, containing $0,1,2,…,p$ predictors.\n",
    "\n",
    "Which of the two models with $k$ predictors is guaranteed to have training RSS no larger than the other model?\n",
    "\n",
    "1. Best Subset\n",
    "2. Forward Stepwise\n",
    "3. They always have the same training RSS\n",
    "4. Not enough information is given to know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 1.\n",
    "</span>\n",
    "\n",
    "Best subset selection may have the smallest test RSS because it takes into account more models than the other methods. However, the other methods might also pick a model with smaller test RSS by sheer luck.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 6.2.R2\n",
    "\n",
    "Which of the two models with $k$ predictors has the smallest test RSS?\n",
    "\n",
    "1. Best Subset\n",
    "2. Forward Stepwise\n",
    "3. They always have the same test RSS\n",
    "4. Not enough information is given to know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 4.\n",
    "</span>\n",
    "\n",
    "We know that Best Subset selection will always have the lowest training RSS (that is how it is defined). That said, we don't know which model will perform better on a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 6.3.R1\n",
    "\n",
    "You are trying to fit a model and are given $p=30$ predictor variables to choose from. Ultimately, you want your model to be interpretable, so you decide to use Best Subset Selection.\n",
    "\n",
    "How many different models will you end up considering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Gyu-Ho's Answer: $2^{30}$.</span>\n",
    "\n",
    "Each predictor can either be included or not included in the model. That means that for each of the 30 variables there are two options. Thus, there are $2^{30}$ potential models.\n",
    "\n",
    "Note: Don’t ever try to fit that many models! It is too many and that is why Best Subset Selection is rarely used in practice for say p=10 or larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 6.3.R2\n",
    "\n",
    "How many would you fit using Forward Selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 466.\n",
    "</span>\n",
    "\n",
    "For Forward Selection, you fit $(p-k)$ models for each $k=0,...p-1$.\n",
    "The expression for the total number of models fit: $1 + \\frac{p(p+1)}{2} = 1 + \\frac{30 * 31}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.4.R1\n",
    "\n",
    "You are fitting a linear model to data assumed to have Gaussian errors. The model has up to $p=5$ predictors and $n=100$ observations. Which of the following is most likely true of the relationship between $C_{p}$ and $AIC$ in terms of using the statistic to select a number of predictors to include?\n",
    "\n",
    "1. $C_{p}$ will select a model with more predictors $AIC$.\n",
    "2. $C_{p}$ will select a model with fewer predictors $AIC$.\n",
    "3. $C_{p}$ will select the same model as $AIC$.\n",
    "4. Not enough information is given to decide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 4.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:red\">\n",
    "3.\n",
    "</span>\n",
    "\n",
    "For **linear models with Gaussian errors**, Cp and AIC and equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.5.R1\n",
    "\n",
    "You are doing a simulation in order to compare the effect of using Cross-Validation or a Validation set. For each iteration of the simulation, you generate new data and then use both Cross-Validation and a Validation set in order to determine the optimal number of predictors. Which of the following is most likely?\n",
    "\n",
    "1. The Cross-Validation method will result in a higher variance of optimal number of predictors.\n",
    "2. The Validation set method will result in a higher variance of optimal number of predictors.\n",
    "3. Both methods will produce results with the same variance of optimal number of predictors.\n",
    "4. Not enough information is given to decide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 2.\n",
    "</span>\n",
    "\n",
    "Cross-Validation is similar to doing a Validation set multiple times and then averaging the answers. As such, we expect it to have lower variance than the Validation set method. This is why Cross-Validation is appealing (especially for small $n$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6.6.R1\n",
    "\n",
    "$\\sqrt{∑^{j=1}_{p} β^{2}_{j}}$ is equivalent to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: L2 norm of β.\n",
    "\n",
    "$\\sqrt{∑^{j=1}_{p} β^{2}_{j}} = \\left\\Vert β \\right\\Vert^2$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.6.R2\n",
    "\n",
    "You perform ridge regression on a problem where your third predictor, $x_{3}$, is measured in dollars. You decide to refit the model after changing $x_{3}$ to be measured in cents. Which of the following is true?:\n",
    "\n",
    "1. $\\hat{β}_{3}$ and $\\hat{y}$ will remain the same.\n",
    "2. $\\hat{β}_{3}$ will change but $\\hat{y}$ will remain the same.\n",
    "3. $\\hat{β}_{3}$ will remain the same but $\\hat{y}$ will change.\n",
    "4. $\\hat{β}_{3}$ and $\\hat{y}$ will both change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 1.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:red\">\n",
    "4.\n",
    "</span>\n",
    "\n",
    "The units of the predictors affects the L2 penalty in ridge regression, and hence $\\hat{β}_{3}$ and $\\hat{y}$ will both change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.7.R1\n",
    "\n",
    "Which of the following is NOT a benefit of the sparsity imposed by the Lasso?\n",
    "\n",
    "1. Sparse models are generally more easy to interperet.\n",
    "2. The Lasso does variable selection by default.\n",
    "3. Using the Lasso penalty helps to decrease the bias of the fits.\n",
    "4. Using the Lasso penalty helps to decrease the variance of the fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 3.\n",
    "</span>\n",
    "\n",
    "Restricting ourselves to simpler models by including a Lasso penalty will generally decrease the variance of the fits at the cost of higher bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.8.R1\n",
    "\n",
    "Which of the following would be the worst metric to use to select $λ$ in the Lasso?\n",
    "\n",
    "1. Cross-Validated error\n",
    "2. Validation set error\n",
    "3. RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 3.\n",
    "</span>\n",
    "\n",
    "RSS would be the worst metric to use because it will cause us to always select the most complicated model. Any of the other metrics could be used, although Cross-Validated error is probably most common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.9.R1\n",
    "\n",
    "We compute the principal components of our p predictor variables. The RSS in a simple linear regression of Y onto the largest principal component will always be no larger than the RSS in a simple regression of Y onto the second largest principal component. True or False? (You may want to watch 6.10 as well before answering - sorry!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: False.\n",
    "</span>\n",
    "\n",
    "Adding more variables reduces the Residual Square Sums (RSS) in a linear model.\n",
    "\n",
    "The answer is simply that we are using the least squares method. Any set of coefficients we choose must give a sum of squared residuals at least as great as for the best fit. Suppose we fit the model with the coefficients of the additional variables set to zero. This is the same as the fit without the additional variables, and as it restricts the coefficients, the sum of squares must be suboptimal except in the unlikely event that the least squares fit has these coefficinetns exactly zero.\n",
    "\n",
    "Principal components are found independently of Y, so we can't know the relationship with Y a priori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.10.R1\n",
    "\n",
    "You are working on a regression problem with many variables, so you decide to do Principal Components Analysis first and then fit the regression to the first 2 principal components. Which of the following would you expect to happen?:\n",
    "\n",
    "1. A subset of the features will be selected.\n",
    "2. Model Bias will decrease relative to the full least squares model.\n",
    "3. Variance of fitted values will decrease relative to the full least squares model.\n",
    "4. Model interpretability will improve relative to the full least squares model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 3.\n",
    "</span>\n",
    "\n",
    "While some forms of dimensional reduction will cause the first or fourth to occur, that is not the case with PCA. When using dimensional reduction we restrict ourselves to simpler models. Thus, we expect bias to increase and variance to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.Q.1\n",
    "\n",
    "Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "\n",
    "$\\sum_{i=1}^n ( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} ) + \\lambda\\sum_{j=1}^{p}\\beta_j^2$\n",
    "\n",
    "for a particular value of $λ$. For each of the following, select the correct answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we increase $λ$ from 0, the **training RSS** will:\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Steadily increase.\n",
    "</span>\n",
    "\n",
    "Increasing $λ$ will force us to fit simpler models. This means that training RSS will steadily increase because we are less able to fit the training data exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we increase $λ$ from 0, the **test RSS** will:\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Decrease initially, and then eventually start increasing in a U shape.\n",
    "</span>\n",
    "\n",
    "At first, we expect test RSS to improve because we are not overfitting our training data anymore. Eventually, we will start fitting models that are too simple to capture the true effects and test RSS will go up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we increase $λ$ from 0, the **variance** will:\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Steadily decrease.\n",
    "</span>\n",
    "\n",
    " Increasing $λ$ will cause us to fit simpler models, which reduces the variance of the fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we increase $λ$ from 0, the **(squared) bias** will:\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Steadily increase.\n",
    "</span>\n",
    "\n",
    "Increasing $λ$ will cause us to fit simpler models, which have larger squared bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we increase $λ$ from 0, the **irreducible error** will:\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Remain constant.\n",
    "</span>\n",
    "\n",
    "Increasing $λ$ will have no effect on irreducible error. By definition, irreducible error is an aspect of the problem and has nothing to do with a particular model being fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.Q.1-1\n",
    "\n",
    "Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "\n",
    "$\\sum_{i=1}^n ( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} )$ subject to $\\sum_{j=1}^{p}|\\beta_j|\\leq s$\n",
    "\n",
    "for a particular value of $λ$. For each of the following, select the correct answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we increase $λ$ from 0, the **training RSS** will:\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Steadily increase.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we increase $λ$ from 0, the **test RSS** will:\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Decrease initially, and then eventually start increasing in a U shape.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we increase $λ$ from 0, the **variance** will:\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Steadily decrease.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we increase $λ$ from 0, the **(squared) bias** will:\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Steadily increase.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we increase $λ$ from 0, the **irreducible error** will:\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Remain constant.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.R.R1\n",
    "\n",
    "One of the functions in the glmnet package is cv.glmnet(). This function, like many functions in R, will return a list object that contains various outputs of interest. What is the name of the component that contains a vector of the mean cross-validated errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n",
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n",
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n",
      "\n",
      "Attaching package: 'pls'\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    loadings\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Libraries have been loaded!\"\n"
     ]
    }
   ],
   "source": [
    "LoadLibraries = function() {\n",
    "    library(MASS)\n",
    "    install.packages(\"ISLR\")\n",
    "    library(ISLR)\n",
    "    install.packages(\"leaps\")\n",
    "    library(leaps)\n",
    "    install.packages(\"pls\")\n",
    "    library(pls)\n",
    "    print(\"Libraries have been loaded!\")\n",
    "}\n",
    "\n",
    "LoadLibraries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'AtBat'</li>\n",
       "\t<li>'Hits'</li>\n",
       "\t<li>'HmRun'</li>\n",
       "\t<li>'Runs'</li>\n",
       "\t<li>'RBI'</li>\n",
       "\t<li>'Walks'</li>\n",
       "\t<li>'Years'</li>\n",
       "\t<li>'CAtBat'</li>\n",
       "\t<li>'CHits'</li>\n",
       "\t<li>'CHmRun'</li>\n",
       "\t<li>'CRuns'</li>\n",
       "\t<li>'CRBI'</li>\n",
       "\t<li>'CWalks'</li>\n",
       "\t<li>'League'</li>\n",
       "\t<li>'Division'</li>\n",
       "\t<li>'PutOuts'</li>\n",
       "\t<li>'Assists'</li>\n",
       "\t<li>'Errors'</li>\n",
       "\t<li>'Salary'</li>\n",
       "\t<li>'NewLeague'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'AtBat'\n",
       "\\item 'Hits'\n",
       "\\item 'HmRun'\n",
       "\\item 'Runs'\n",
       "\\item 'RBI'\n",
       "\\item 'Walks'\n",
       "\\item 'Years'\n",
       "\\item 'CAtBat'\n",
       "\\item 'CHits'\n",
       "\\item 'CHmRun'\n",
       "\\item 'CRuns'\n",
       "\\item 'CRBI'\n",
       "\\item 'CWalks'\n",
       "\\item 'League'\n",
       "\\item 'Division'\n",
       "\\item 'PutOuts'\n",
       "\\item 'Assists'\n",
       "\\item 'Errors'\n",
       "\\item 'Salary'\n",
       "\\item 'NewLeague'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'AtBat'\n",
       "2. 'Hits'\n",
       "3. 'HmRun'\n",
       "4. 'Runs'\n",
       "5. 'RBI'\n",
       "6. 'Walks'\n",
       "7. 'Years'\n",
       "8. 'CAtBat'\n",
       "9. 'CHits'\n",
       "10. 'CHmRun'\n",
       "11. 'CRuns'\n",
       "12. 'CRBI'\n",
       "13. 'CWalks'\n",
       "14. 'League'\n",
       "15. 'Division'\n",
       "16. 'PutOuts'\n",
       "17. 'Assists'\n",
       "18. 'Errors'\n",
       "19. 'Salary'\n",
       "20. 'NewLeague'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n",
       " [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n",
       "[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n",
       "[19] \"Salary\"    \"NewLeague\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>322</li>\n",
       "\t<li>20</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 322\n",
       "\\item 20\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 322\n",
       "2. 20\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 322  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>263</li>\n",
       "\t<li>20</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 263\n",
       "\\item 20\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 263\n",
       "2. 20\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 263  20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names(Hitters)\n",
    "dim(Hitters)\n",
    "Hitters = na.omit(Hitters)\n",
    "dim(Hitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "Loading required package: foreach\n",
      "Loaded glmnet 2.0-5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>20</li>\n",
       "\t<li>100</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 20\n",
       "\\item 100\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 20\n",
       "2. 100\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  20 100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(glmnet)\n",
    "\n",
    "# model.matrix to produce a matrix with 19 predictors\n",
    "# also automatically transforms any qualitative variables into dummy variables\n",
    "x = model.matrix(Salary~., Hitters)[,-1]\n",
    "y = Hitters$Salary\n",
    "grid = 10^seq(10, -2, length=100)\n",
    "\n",
    "# alpha=0 for ridge regression\n",
    "# alpha=1 for lasso\n",
    "# automatically standardize variables\n",
    "ridge.mod = glmnet(x, y, alpha=0, lambda=grid)\n",
    "dim(coef(ridge.mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split samples into training set and test set\n",
    "# to estimate test error of ridge regression, lasso\n",
    "set.seed(1)\n",
    "train = sample(1:nrow(x), nrow(x)/2)\n",
    "test = (-train)\n",
    "y.test = y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'lambda'</li>\n",
       "\t<li>'cvm'</li>\n",
       "\t<li>'cvsd'</li>\n",
       "\t<li>'cvup'</li>\n",
       "\t<li>'cvlo'</li>\n",
       "\t<li>'nzero'</li>\n",
       "\t<li>'name'</li>\n",
       "\t<li>'glmnet.fit'</li>\n",
       "\t<li>'lambda.min'</li>\n",
       "\t<li>'lambda.1se'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'lambda'\n",
       "\\item 'cvm'\n",
       "\\item 'cvsd'\n",
       "\\item 'cvup'\n",
       "\\item 'cvlo'\n",
       "\\item 'nzero'\n",
       "\\item 'name'\n",
       "\\item 'glmnet.fit'\n",
       "\\item 'lambda.min'\n",
       "\\item 'lambda.1se'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'lambda'\n",
       "2. 'cvm'\n",
       "3. 'cvsd'\n",
       "4. 'cvup'\n",
       "5. 'cvlo'\n",
       "6. 'nzero'\n",
       "7. 'name'\n",
       "8. 'glmnet.fit'\n",
       "9. 'lambda.min'\n",
       "10. 'lambda.1se'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"lambda\"     \"cvm\"        \"cvsd\"       \"cvup\"       \"cvlo\"      \n",
       " [6] \"nzero\"      \"name\"       \"glmnet.fit\" \"lambda.min\" \"lambda.1se\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use cross-validation to choose λ\n",
    "set.seed(1)\n",
    "cv.out = cv.glmnet(x[train,], y[train], alpha=0)\n",
    "names(cv.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "211.741584781282"
      ],
      "text/latex": [
       "211.741584781282"
      ],
      "text/markdown": [
       "211.741584781282"
      ],
      "text/plain": [
       "[1] 211.7416"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>214354.303637251</li>\n",
       "\t<li>213164.708864405</li>\n",
       "\t<li>212292.015886432</li>\n",
       "\t<li>212085.979574303</li>\n",
       "\t<li>211861.028528945</li>\n",
       "\t<li>211615.551030778</li>\n",
       "\t<li>211347.819509483</li>\n",
       "\t<li>211055.989690819</li>\n",
       "\t<li>210738.098514156</li>\n",
       "\t<li>210392.063370548</li>\n",
       "\t<li>210015.682995141</li>\n",
       "\t<li>209606.640393825</li>\n",
       "\t<li>209162.508230901</li>\n",
       "\t<li>208680.758274873</li>\n",
       "\t<li>208158.757947734</li>\n",
       "\t<li>207593.705024591</li>\n",
       "\t<li>206982.972523307</li>\n",
       "\t<li>206323.835151519</li>\n",
       "\t<li>205613.444938001</li>\n",
       "\t<li>204849.0620957</li>\n",
       "\t<li>204028.029452135</li>\n",
       "\t<li>203147.823047691</li>\n",
       "\t<li>202206.109641969</li>\n",
       "\t<li>201200.810935912</li>\n",
       "\t<li>200130.173987882</li>\n",
       "\t<li>198992.846902772</li>\n",
       "\t<li>197787.958409088</li>\n",
       "\t<li>196515.199421301</li>\n",
       "\t<li>195174.904134216</li>\n",
       "\t<li>193768.127643148</li>\n",
       "\t<li>192296.716569609</li>\n",
       "\t<li>190763.36874804</li>\n",
       "\t<li>189171.6777536</li>\n",
       "\t<li>187526.153569562</li>\n",
       "\t<li>185832.258691162</li>\n",
       "\t<li>184096.387312978</li>\n",
       "\t<li>182325.62228014</li>\n",
       "\t<li>180527.736955726</li>\n",
       "\t<li>178711.408641509</li>\n",
       "\t<li>176885.679248302</li>\n",
       "\t<li>175059.90957603</li>\n",
       "\t<li>173243.589909034</li>\n",
       "\t<li>171446.139190335</li>\n",
       "\t<li>169676.701409249</li>\n",
       "\t<li>167943.948460026</li>\n",
       "\t<li>166255.898693723</li>\n",
       "\t<li>164619.813209113</li>\n",
       "\t<li>163042.062502039</li>\n",
       "\t<li>161527.575079549</li>\n",
       "\t<li>160080.311091669</li>\n",
       "\t<li>158703.366985309</li>\n",
       "\t<li>157399.333113961</li>\n",
       "\t<li>156169.139851723</li>\n",
       "\t<li>155012.915367284</li>\n",
       "\t<li>153930.109807678</li>\n",
       "\t<li>152919.53912513</li>\n",
       "\t<li>151979.517997744</li>\n",
       "\t<li>151107.992056028</li>\n",
       "\t<li>150302.663700497</li>\n",
       "\t<li>149561.168026721</li>\n",
       "\t<li>148880.594649595</li>\n",
       "\t<li>148258.567388932</li>\n",
       "\t<li>147695.308013481</li>\n",
       "\t<li>147186.411913869</li>\n",
       "\t<li>146727.433461154</li>\n",
       "\t<li>146320.375377957</li>\n",
       "\t<li>145961.813983003</li>\n",
       "\t<li>145648.394285883</li>\n",
       "\t<li>145378.467172561</li>\n",
       "\t<li>145155.58023841</li>\n",
       "\t<li>144974.873035186</li>\n",
       "\t<li>144831.441079251</li>\n",
       "\t<li>144726.027706479</li>\n",
       "\t<li>144654.900147812</li>\n",
       "\t<li>144617.044677928</li>\n",
       "\t<li>144606.301830691</li>\n",
       "\t<li>144623.530938551</li>\n",
       "\t<li>144664.407824805</li>\n",
       "\t<li>144728.059578698</li>\n",
       "\t<li>144808.98990538</li>\n",
       "\t<li>144907.142629995</li>\n",
       "\t<li>145016.549259666</li>\n",
       "\t<li>145135.626680521</li>\n",
       "\t<li>145265.863387186</li>\n",
       "\t<li>145397.120813286</li>\n",
       "\t<li>145534.406552462</li>\n",
       "\t<li>145675.893264413</li>\n",
       "\t<li>145811.569975284</li>\n",
       "\t<li>145946.620051613</li>\n",
       "\t<li>146078.020812919</li>\n",
       "\t<li>146206.383507617</li>\n",
       "\t<li>146327.728292901</li>\n",
       "\t<li>146442.204136978</li>\n",
       "\t<li>146550.1860599</li>\n",
       "\t<li>146649.689588485</li>\n",
       "\t<li>146741.094410638</li>\n",
       "\t<li>146824.430621866</li>\n",
       "\t<li>146899.156499292</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 214354.303637251\n",
       "\\item 213164.708864405\n",
       "\\item 212292.015886432\n",
       "\\item 212085.979574303\n",
       "\\item 211861.028528945\n",
       "\\item 211615.551030778\n",
       "\\item 211347.819509483\n",
       "\\item 211055.989690819\n",
       "\\item 210738.098514156\n",
       "\\item 210392.063370548\n",
       "\\item 210015.682995141\n",
       "\\item 209606.640393825\n",
       "\\item 209162.508230901\n",
       "\\item 208680.758274873\n",
       "\\item 208158.757947734\n",
       "\\item 207593.705024591\n",
       "\\item 206982.972523307\n",
       "\\item 206323.835151519\n",
       "\\item 205613.444938001\n",
       "\\item 204849.0620957\n",
       "\\item 204028.029452135\n",
       "\\item 203147.823047691\n",
       "\\item 202206.109641969\n",
       "\\item 201200.810935912\n",
       "\\item 200130.173987882\n",
       "\\item 198992.846902772\n",
       "\\item 197787.958409088\n",
       "\\item 196515.199421301\n",
       "\\item 195174.904134216\n",
       "\\item 193768.127643148\n",
       "\\item 192296.716569609\n",
       "\\item 190763.36874804\n",
       "\\item 189171.6777536\n",
       "\\item 187526.153569562\n",
       "\\item 185832.258691162\n",
       "\\item 184096.387312978\n",
       "\\item 182325.62228014\n",
       "\\item 180527.736955726\n",
       "\\item 178711.408641509\n",
       "\\item 176885.679248302\n",
       "\\item 175059.90957603\n",
       "\\item 173243.589909034\n",
       "\\item 171446.139190335\n",
       "\\item 169676.701409249\n",
       "\\item 167943.948460026\n",
       "\\item 166255.898693723\n",
       "\\item 164619.813209113\n",
       "\\item 163042.062502039\n",
       "\\item 161527.575079549\n",
       "\\item 160080.311091669\n",
       "\\item 158703.366985309\n",
       "\\item 157399.333113961\n",
       "\\item 156169.139851723\n",
       "\\item 155012.915367284\n",
       "\\item 153930.109807678\n",
       "\\item 152919.53912513\n",
       "\\item 151979.517997744\n",
       "\\item 151107.992056028\n",
       "\\item 150302.663700497\n",
       "\\item 149561.168026721\n",
       "\\item 148880.594649595\n",
       "\\item 148258.567388932\n",
       "\\item 147695.308013481\n",
       "\\item 147186.411913869\n",
       "\\item 146727.433461154\n",
       "\\item 146320.375377957\n",
       "\\item 145961.813983003\n",
       "\\item 145648.394285883\n",
       "\\item 145378.467172561\n",
       "\\item 145155.58023841\n",
       "\\item 144974.873035186\n",
       "\\item 144831.441079251\n",
       "\\item 144726.027706479\n",
       "\\item 144654.900147812\n",
       "\\item 144617.044677928\n",
       "\\item 144606.301830691\n",
       "\\item 144623.530938551\n",
       "\\item 144664.407824805\n",
       "\\item 144728.059578698\n",
       "\\item 144808.98990538\n",
       "\\item 144907.142629995\n",
       "\\item 145016.549259666\n",
       "\\item 145135.626680521\n",
       "\\item 145265.863387186\n",
       "\\item 145397.120813286\n",
       "\\item 145534.406552462\n",
       "\\item 145675.893264413\n",
       "\\item 145811.569975284\n",
       "\\item 145946.620051613\n",
       "\\item 146078.020812919\n",
       "\\item 146206.383507617\n",
       "\\item 146327.728292901\n",
       "\\item 146442.204136978\n",
       "\\item 146550.1860599\n",
       "\\item 146649.689588485\n",
       "\\item 146741.094410638\n",
       "\\item 146824.430621866\n",
       "\\item 146899.156499292\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 214354.303637251\n",
       "2. 213164.708864405\n",
       "3. 212292.015886432\n",
       "4. 212085.979574303\n",
       "5. 211861.028528945\n",
       "6. 211615.551030778\n",
       "7. 211347.819509483\n",
       "8. 211055.989690819\n",
       "9. 210738.098514156\n",
       "10. 210392.063370548\n",
       "11. 210015.682995141\n",
       "12. 209606.640393825\n",
       "13. 209162.508230901\n",
       "14. 208680.758274873\n",
       "15. 208158.757947734\n",
       "16. 207593.705024591\n",
       "17. 206982.972523307\n",
       "18. 206323.835151519\n",
       "19. 205613.444938001\n",
       "20. 204849.0620957\n",
       "21. 204028.029452135\n",
       "22. 203147.823047691\n",
       "23. 202206.109641969\n",
       "24. 201200.810935912\n",
       "25. 200130.173987882\n",
       "26. 198992.846902772\n",
       "27. 197787.958409088\n",
       "28. 196515.199421301\n",
       "29. 195174.904134216\n",
       "30. 193768.127643148\n",
       "31. 192296.716569609\n",
       "32. 190763.36874804\n",
       "33. 189171.6777536\n",
       "34. 187526.153569562\n",
       "35. 185832.258691162\n",
       "36. 184096.387312978\n",
       "37. 182325.62228014\n",
       "38. 180527.736955726\n",
       "39. 178711.408641509\n",
       "40. 176885.679248302\n",
       "41. 175059.90957603\n",
       "42. 173243.589909034\n",
       "43. 171446.139190335\n",
       "44. 169676.701409249\n",
       "45. 167943.948460026\n",
       "46. 166255.898693723\n",
       "47. 164619.813209113\n",
       "48. 163042.062502039\n",
       "49. 161527.575079549\n",
       "50. 160080.311091669\n",
       "51. 158703.366985309\n",
       "52. 157399.333113961\n",
       "53. 156169.139851723\n",
       "54. 155012.915367284\n",
       "55. 153930.109807678\n",
       "56. 152919.53912513\n",
       "57. 151979.517997744\n",
       "58. 151107.992056028\n",
       "59. 150302.663700497\n",
       "60. 149561.168026721\n",
       "61. 148880.594649595\n",
       "62. 148258.567388932\n",
       "63. 147695.308013481\n",
       "64. 147186.411913869\n",
       "65. 146727.433461154\n",
       "66. 146320.375377957\n",
       "67. 145961.813983003\n",
       "68. 145648.394285883\n",
       "69. 145378.467172561\n",
       "70. 145155.58023841\n",
       "71. 144974.873035186\n",
       "72. 144831.441079251\n",
       "73. 144726.027706479\n",
       "74. 144654.900147812\n",
       "75. 144617.044677928\n",
       "76. 144606.301830691\n",
       "77. 144623.530938551\n",
       "78. 144664.407824805\n",
       "79. 144728.059578698\n",
       "80. 144808.98990538\n",
       "81. 144907.142629995\n",
       "82. 145016.549259666\n",
       "83. 145135.626680521\n",
       "84. 145265.863387186\n",
       "85. 145397.120813286\n",
       "86. 145534.406552462\n",
       "87. 145675.893264413\n",
       "88. 145811.569975284\n",
       "89. 145946.620051613\n",
       "90. 146078.020812919\n",
       "91. 146206.383507617\n",
       "92. 146327.728292901\n",
       "93. 146442.204136978\n",
       "94. 146550.1860599\n",
       "95. 146649.689588485\n",
       "96. 146741.094410638\n",
       "97. 146824.430621866\n",
       "98. 146899.156499292\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 214354.3 213164.7 212292.0 212086.0 211861.0 211615.6 211347.8 211056.0\n",
       " [9] 210738.1 210392.1 210015.7 209606.6 209162.5 208680.8 208158.8 207593.7\n",
       "[17] 206983.0 206323.8 205613.4 204849.1 204028.0 203147.8 202206.1 201200.8\n",
       "[25] 200130.2 198992.8 197788.0 196515.2 195174.9 193768.1 192296.7 190763.4\n",
       "[33] 189171.7 187526.2 185832.3 184096.4 182325.6 180527.7 178711.4 176885.7\n",
       "[41] 175059.9 173243.6 171446.1 169676.7 167943.9 166255.9 164619.8 163042.1\n",
       "[49] 161527.6 160080.3 158703.4 157399.3 156169.1 155012.9 153930.1 152919.5\n",
       "[57] 151979.5 151108.0 150302.7 149561.2 148880.6 148258.6 147695.3 147186.4\n",
       "[65] 146727.4 146320.4 145961.8 145648.4 145378.5 145155.6 144974.9 144831.4\n",
       "[73] 144726.0 144654.9 144617.0 144606.3 144623.5 144664.4 144728.1 144809.0\n",
       "[81] 144907.1 145016.5 145135.6 145265.9 145397.1 145534.4 145675.9 145811.6\n",
       "[89] 145946.6 146078.0 146206.4 146327.7 146442.2 146550.2 146649.7 146741.1\n",
       "[97] 146824.4 146899.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lambda.min is the value of λ that gives minimum mean cross-validated error\n",
    "cv.out$lambda.min\n",
    "\n",
    "# contains a vector of the mean cross-validated errors\n",
    "cv.out$cvm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
