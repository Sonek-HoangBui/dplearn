{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px;\"><b>Stats216v: Statistical Learning</b></div>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center\">Stanford University</div>\n",
    "<div style=\"text-align: center\">Summer 2017</div>\n",
    "<div style=\"text-align: center\">Gyu-Ho Lee (<a href=\"mailto:gyuhox@gmail.com\">gyuhox@gmail.com</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.1.R1\n",
    "\n",
    "You are analyzing a dataset where each observation is an age, height, length, and width of a particular turtle. You want to know if the data can be well described by fewer than four dimensions (maybe for plotting), so you decide to do Principal Component Analysis. Which of the following is most likely to be the loadings of the first Principal Component?\n",
    "\n",
    "1. (1, 1, 1, 1)\n",
    "2. (.5, .5, .5, .5)\n",
    "3. (.71, -.71, 0, 0)\n",
    "4. (1, -1, -1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 2.\n",
    "</span>\n",
    "\n",
    "We know that options 1 and 4 cannot be right because the sum of the squared loadings exceeds 1. The second option is most likely correct because we expect all four variables to be positively correlated with each-other.\n",
    "\n",
    "Note that it is fairly common for the loadings of the first principal component to all have the same sign. In such a case, the principal component is often referred to as a size component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.2.R1\n",
    "\n",
    "Suppose we a data set where each data point represents a single student's scores on a math test, a physics test, a reading comprehension test, and a vocabulary test.\n",
    "\n",
    "We find the first two principal components, which capture 90% of the variability in the data, and interpret their loadings. We conclude that the first principal component represents overall academic ability, and the second represents a contrast between quantitative ability and verbal ability.\n",
    "\n",
    "What loadings would be consistent with that interpretation? Choose all that apply.\n",
    "\n",
    "1. (0.5, 0.5, 0.5, 0.5) and (0.71, 0.71, 0, 0)\n",
    "2. (0.5, 0.5, 0.5, 0.5) and (0, 0, -0.71, -0.71)\n",
    "3. (0.5, 0.5, 0.5, 0.5) and (0.5, 0.5, -0.5, -0.5)\n",
    "4. (0.5, 0.5, 0.5, 0.5) and (-0.5, -0.5, 0.5, 0.5)\n",
    "5. (0.71, 0.71, 0, 0) and (0, 0, 0.71, 0.71)\n",
    "6. (0.71, 0, -0.71, 0) and (0, 0.71, 0, -0.71)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 3, 4.\n",
    "</span>\n",
    "\n",
    "For the first two choices, the two loading vectors are not orthogonal. For the fifth and sixth choices, the first set of loadings only has to do with two specific tests. For the third and fourth pairs of loadings, the first component is proportional to average score, and the second component measures the difference between the first pair of scores and the second pair of scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 10.3.R1\n",
    "\n",
    "True or False: If we use k-means clustering, will we get the same cluster assignments for each point, whether or not we standardize the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: False.\n",
    "</span>\n",
    "\n",
    "The points are assigned to centroids using Euclidean distance. If we change the scaling of one variable, e.g. by dividing it by 10, then that variable will matter less in determining Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 10.4.R1\n",
    "\n",
    "True or False: If we cut the dendrogram at a lower point, we will tend to get more clusters (and cannot get fewer clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Gyu-Ho's Answer: True.</span>\n",
    "\n",
    "After cutting the dendrogram at threshold t, we keep all the joins with linkage distance less than t and discard the joins with larger linkage distance. Thus, decreasing the threshold gives us fewer joins, and thus more clusters. If, in decreasing the threshold, we don't cross a junction of the dendrogram, the number of clusters will remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 10.5.R1\n",
    "\n",
    "In the heat map for breast cancer data, which of the following depended on the output of hierarchical clustering?\n",
    "\n",
    "1. The ordering of the rows\n",
    "2. The ordering of the columns\n",
    "3. The coloring of the cells as red or green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 1, 2.\n",
    "</span>\n",
    "\n",
    "The dendrograms obtained from hierarchical clustering were used to order the rows and columns. The coloring of the cells was based on gene expression, but without the hierarchical clustering step, the heat map would not have looked like anything meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Libraries have been loaded!\"\n"
     ]
    }
   ],
   "source": [
    "LoadLibraries = function() {\n",
    "    library(MASS)\n",
    "    install.packages(\"ISLR\")\n",
    "    library(ISLR)\n",
    "    print(\"Libraries have been loaded!\")\n",
    "}\n",
    "\n",
    "LoadLibraries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.R.1\n",
    "\n",
    "Suppose we want to fit a linear regression, but the number of variables is much larger than the number of observations. In some cases, we may improve the fit by reducing the dimension of the features before.\n",
    "\n",
    "In this problem, we use a data set with n = 300 and p = 200, so we have more observations than variables, but not by much. Load the data `x`, `y`, `x.test`, and `y.test` from `10.R.RData`.\n",
    "\n",
    "First, concatenate `x` and `x.test` using the rbind functions and perform a principal components analysis on the concatenated data frame (use the `\"scale=TRUE\"` option). To within 10% relative error, what proportion of the variance is explained by the first five principal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.R.2\n",
    "\n",
    "The previous answer suggests that a relatively small number of \"latent variables\" account for a substantial fraction of the features' variability. We might believe that these latent variables are more important than linear combinations of the features that have low variance.\n",
    "\n",
    "We can try forgetting about the raw features and using the first five principal components (computed on `rbind(x,x.test)`) instead as low-dimensional derived features. What is the mean-squared test error if we regress y on the first five principal components, and use the resulting model to predict `y.test`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.R.3\n",
    "\n",
    "Now, try an OLS linear regression of y on the matrix `x`. What is the mean squared predition error if we use the fitted model to predict `y.test` from `x.test`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.Q.1\n",
    "\n",
    "K-Means is a seemingly complicated clustering algorithms. Here is a simpler one:\n",
    "\n",
    "Given k, the number of clusters, and n, the number of observations, try all possible assignments of the n observations into k clusters. Then, select one of the assignments that minimizes Within-Cluster Variation as defined on page 30.\n",
    "\n",
    "Assume that you implemented the most naive version of the above algorithm. Here, by naive we mean that you try all possible assignments even though some of them might be redundant (for example, the algorithm tries assigning all of the observations to cluster 1 and it also tries to assign them all to cluster 2 even though those are effectively the same solution).\n",
    "\n",
    "In terms of n and k, how many potential solutions will your algorithm try?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: $k^n$.\n",
    "</span>\n",
    "\n",
    "For each of the n observations we have k options for assignment. Each of the assignments is done independently, so $k^n$.\n",
    "\n",
    "Note, the exponential explosion in the number of potential solutions is the reason we need to use greedy algorithms like K-Means in order to perform clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
