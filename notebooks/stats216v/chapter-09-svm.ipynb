{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px;\"><b>Stats216v: Statistical Learning</b></div>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center\">Stanford University</div>\n",
    "<div style=\"text-align: center\">Summer 2017</div>\n",
    "<div style=\"text-align: center\">Gyu-Ho Lee (<a href=\"mailto:gyuhox@gmail.com\">gyuhox@gmail.com</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.1.R1\n",
    "\n",
    "If $\\beta$ is not a unit vector but instead has length 2, then $\\sum_{j=1}^p \\beta_j X_j$ is\n",
    "\n",
    "1. twice the signed Euclidean distance from the separating hyperplane $\\sum_{j=1}^p \\beta_j X_j = 0$\n",
    "2. half the signed Euclidean distance from X to the separating hyperplane\n",
    "3. exactly the signed Euclidean distance from the separating hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 1. It is handy to assume that $\\beta$ is a unit vector, and geometrically it is the normal vector to the hyperplane.\n",
    "</span>\n",
    "\n",
    "We know $\\beta' = \\frac{1}{2}\\beta$ has length 1, so it is a unit vector in the same direction as $\\beta$. Therefore, $\\sum_{j=1}^p \\beta_j X_j = 2\\sum_{j=1}^p \\beta'_j X_j$, where $\\sum_{j=1}^p \\beta'_j X_j$ is the Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.2.R1\n",
    "\n",
    "If we increase C (the error budget) in an SVM, do you expect the standard error of $\\beta$ to increase or decrease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: C is larger, then the margin is wider and we allow more violations to it. So higher bias but lower variance. The standard deviation of the mean is the square root of the variance (the average squared deviation from the mean). The standard error of the mean is the expected value of the standard deviation of means of several samples. Therefore C is larger, then lower variance, then standard error of $\\beta$ **decreases**.\n",
    "</span>\n",
    "\n",
    "Increasing C makes the margin \"softer,\" so that the orientation of the separating hyperplane is influenced by more points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 9.3.R1\n",
    "\n",
    "True or False: If no linear boundary can perfectly classify all the training data, this means we need to use a feature expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: False.\n",
    "</span>\n",
    "\n",
    "As in any statistical problem, we will always do better on the training data if we use a feature expansion, but that doesn't mean we will improve the test error. Not all regression lines should perfectly interpolate all the training points, and not all classifiers should perfectly classify all the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 9.3.R2\n",
    "\n",
    "True or False: The computational effort required to solve a kernel support vector machine becomes greater and greater as the dimension of the basis increases. (Note: the dimension of the basis is not the same as p, the dimension of the predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Gyu-Ho's Answer: False.</span>\n",
    "\n",
    "The beauty of the \"kernel trick\" is that, even if there is an infinite-dimensional basis, we need only look at the n^2 inner products between training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 9.4.R1\n",
    "\n",
    "Recall that we obtain the ROC curve by classifying test points based on whether $\\hat f(x) > t$, and varying $t$.\n",
    "\n",
    "How large is the AUC (area under the ROC curve) for a classifier based on a completely random function $\\hat f(x)$ (that is, one for which the orderings of the $\\hat f(x_i)$ are completely random)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:red\">\n",
    "0.5.\n",
    "</span>\n",
    "\n",
    "If $\\hat f(x)$ is completely random, then $\\hat f(x_i)$ (and therefore the prediction for $y_i$) has nothing to do with $y_i$. Thus, the true positive rate and the false positive rate are both equal to the overall positive rate, and the ROC curve hugs the 45-degree line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Libraries have been loaded!\"\n"
     ]
    }
   ],
   "source": [
    "LoadLibraries = function() {\n",
    "    library(MASS)\n",
    "    install.packages(\"e1071\")\n",
    "    library(e1071)\n",
    "    print(\"Libraries have been loaded!\")\n",
    "}\n",
    "\n",
    "LoadLibraries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.5\n",
    "\n",
    "In this problem, you will use simulation to evaluate (by Monte Carlo) the expected misclassification error rate given a particular generating model. Let $y_i$ be equally divided between classes 0 and 1, and let $x_i \\in \\mathbb{R}^{10}$ be normally distributed.\n",
    "\n",
    "Given $y_i=0$, $x_i \\sim N_{10}(0, I_{10})$. Given $y_i=1$, $x_i \\sim N_{10}( \\mu, I_{10})$ with $\\mu = (1,1,1,1,1,0,0,0,0,0)$.\n",
    "\n",
    "Now, we would like to know the expected test error rate if we fit an SVM to a sample of 50 random training points from class 1 and 50 more from class 0.  We can calculate this to high precision by 1) generating a random training sample to train on, 2) evaluating the number of mistakes we make on a large test set, and then 3) repeating (1-2) many times and averaging the error rate for each trial.\n",
    "\n",
    "Aside: in real life don't know the generating distribution, so we have to use resampling methods instead of the procedure described above.\n",
    "\n",
    "For all of the following, please enter your error rate as a number between zero and 1 (e.g., 0.21 instead of 21 if the error rate is 21%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.R.1\n",
    "\n",
    "Use svm in the e1071 package with the default settings (the default kernel is a radial kernel). What is the expected test error rate of this method (to within 10%)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.R.2\n",
    "\n",
    "Now fit an svm with a linear kernel (kernel = \"linear\"). What is the expected test error rate to within 10%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.R.3\n",
    "\n",
    "What is the expected test error for logistic regression? (to within 10%)\n",
    "\n",
    "(Don't worry if you get errors saying the logistic regression did not converge.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.Q.1\n",
    "\n",
    "Suppose that after our computer works for an hour to fit an SVM on a large data set, we notice that $x_4$, the feature vector for the fourth example, was recorded incorrectly (say, one of the decimal points is obviously in the wrong place).\n",
    "\n",
    "However, your co-worker notices that the pair $(x_4, y_4)$ did not turn out to be a support point in the original fit. He says there is no need to re-fit the SVM on the corrected data set, because changing the value of a non-support point can't possibly change the fit.\n",
    "\n",
    "Is your co-worker correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: No.\n",
    "</span>\n",
    "\n",
    "When we change $x_4$, the fourth example might become a support point; if so, the fit may change. However, we could check whether $x_4$,$y_4$ is still not a support point even after correcting the value. If so, then we really don't need to re-fit the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
