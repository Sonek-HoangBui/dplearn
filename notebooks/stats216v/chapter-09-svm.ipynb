{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px;\"><b>Stats216v: Statistical Learning</b></div>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center\">Stanford University</div>\n",
    "<div style=\"text-align: center\">Summer 2017</div>\n",
    "<div style=\"text-align: center\">Gyu-Ho Lee (<a href=\"mailto:gyuhox@gmail.com\">gyuhox@gmail.com</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.1.R1\n",
    "\n",
    "If $\\beta$ is not a unit vector but instead has length 2, then $\\sum_{j=1}^p \\beta_j X_j$ is\n",
    "\n",
    "1. twice the signed Euclidean distance from the separating hyperplane $\\sum_{j=1}^p \\beta_j X_j = 0$\n",
    "2. half the signed Euclidean distance from X to the separating hyperplane\n",
    "3. exactly the signed Euclidean distance from the separating hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 1. It is handy to assume that $\\beta$ is a unit vector, and geometrically it is the normal vector to the hyperplane.\n",
    "</span>\n",
    "\n",
    "We know $\\beta' = \\frac{1}{2}\\beta$ has length 1, so it is a unit vector in the same direction as $\\beta$. Therefore, $\\sum_{j=1}^p \\beta_j X_j = 2\\sum_{j=1}^p \\beta'_j X_j$, where $\\sum_{j=1}^p \\beta'_j X_j$ is the Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.2.R1\n",
    "\n",
    "If we increase C (the error budget) in an SVM, do you expect the standard error of $\\beta$ to increase or decrease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: C is larger, then the margin is wider and we allow more violations to it. So higher bias but lower variance. The standard deviation of the mean is the square root of the variance (the average squared deviation from the mean). The standard error of the mean is the expected value of the standard deviation of means of several samples. Therefore C is larger, then lower variance, then standard error of $\\beta$ **decreases**.\n",
    "</span>\n",
    "\n",
    "Increasing C makes the margin \"softer,\" so that the orientation of the separating hyperplane is influenced by more points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 9.3.R1\n",
    "\n",
    "True or False: If no linear boundary can perfectly classify all the training data, this means we need to use a feature expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: False.\n",
    "</span>\n",
    "\n",
    "As in any statistical problem, we will always do better on the training data if we use a feature expansion, but that doesn't mean we will improve the test error. Not all regression lines should perfectly interpolate all the training points, and not all classifiers should perfectly classify all the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 9.3.R2\n",
    "\n",
    "True or False: The computational effort required to solve a kernel support vector machine becomes greater and greater as the dimension of the basis increases. (Note: the dimension of the basis is not the same as p, the dimension of the predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Gyu-Ho's Answer: False.</span>\n",
    "\n",
    "The beauty of the \"kernel trick\" is that, even if there is an infinite-dimensional basis, we need only look at the n^2 inner products between training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 9.4.R1\n",
    "\n",
    "Recall that we obtain the ROC curve by classifying test points based on whether $\\hat f(x) > t$, and varying $t$.\n",
    "\n",
    "How large is the AUC (area under the ROC curve) for a classifier based on a completely random function $\\hat f(x)$ (that is, one for which the orderings of the $\\hat f(x_i)$ are completely random)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:red\">\n",
    "0.5.\n",
    "</span>\n",
    "\n",
    "If $\\hat f(x)$ is completely random, then $\\hat f(x_i)$ (and therefore the prediction for $y_i$) has nothing to do with $y_i$. Thus, the true positive rate and the false positive rate are both equal to the overall positive rate, and the ROC curve hugs the 45-degree line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.R\n",
    "\n",
    "In which of the following problems is Case/Control Sampling LEAST likely to make a positive impact?\n",
    "\n",
    "1. Predicting a shopper's gender based on the products they buy\n",
    "2. Finding predictors for a certain type of cancer\n",
    "3. Predicting if an email is Spam or Not Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 1.\n",
    "</span>\n",
    "\n",
    "Case/Control sampling is most effective when the prior probabilities of the classes are very unequal. We expect this to be the case for the cancer and spam problems, but not the gender problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.5.R1\n",
    "\n",
    "Suppose that in Ad Clicks (a problem where you try to model if a user will click on a particular ad) it is well known that the majority of the time an ad is shown it will not be clicked. What is another way of saying that?\n",
    "\n",
    "1. Ad Clicks have a low Prior Probability.\n",
    "2. Ad Clicks have a high Prior Probability.\n",
    "3. Ad Clicks have a low Density.\n",
    "4. Ad Clicks have a high Density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 1.\n",
    "</span>\n",
    "\n",
    "Whether or not an ad gets clicked is a Qualitative Variable. Thus, it does not have a density. The Prior Probability of Ad Clicks is low because most ads are not clicked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.6.R1\n",
    "\n",
    "Which of the following is NOT a linear function in x:\n",
    "\n",
    "1. $f(x) = a + b^{2} x$\n",
    "2. The discriminant function from LDA\n",
    "3. $δ_{k}(x) = x \\frac{μ_{k}}{σ^{2}} - \\frac{μ^{2}_{k}}{2σ^{2}} + log(π_{k})$\n",
    "4. logit$(P(y=1|x))$ where $P(y=1|x)$ is as in logistic regression\n",
    "5. $P(y=1|x)$ from logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 5.\n",
    "</span>\n",
    "\n",
    "$P(y=1|x)$ from logistic regression is not linear because it involves both an exponential function of x and a ratio. Notice that $f(x) = a + b^{2} x$ is not a linear function of b, but is a linear function of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.7.R1\n",
    "\n",
    "Why does Total Error keep going down on the graph on page 34 of the notes, even though the False Negative Rate increases?\n",
    "\n",
    "<img src=\"./chapter-04-classification-3.png\" alt=\"chapter-04-classification-3.png\" style=\"width: 450px;\"/>\n",
    "\n",
    "1. The False Negative Rate does not affect Total Error.\n",
    "2. A higher False Negative Rate generally decreases Total Error.\n",
    "3. Positive responses are so uncommon that the False Negatives make up only a small portion of the Total Error.\n",
    "4. All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 3.\n",
    "</span>\n",
    "\n",
    "The Total Error is a weighted average of the False Positive Rate and False Negative Rate. The weights are determined by the Prior Probabilities of Positive and Negative Responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.R1\n",
    "\n",
    "Which of the following statements best explains the relationship between Quadratic Discriminant Analysis and naive Bayes with Gaussian distributions in each class?\n",
    "\n",
    "1. Quadratic Discriminant Analysis is a more flexible class of models than naive Bayes\n",
    "2. Quadratic Discriminant Analysis is a less flexible class of models than naive Bayes\n",
    "3. Quadratic Discriminant Analysis is an equivalently flexible class of models to naive Bayes\n",
    "4. For some problems Quadratic Discriminant Analysis is more flexible than naive Bayes, for others the opposite is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 1.\n",
    "</span>\n",
    "\n",
    "With Gaussian distributions, naive Bayes is equivalent to Quadratic Discriminant Analysis with the additional requirement that each class covariance matrix $Σ_{k}$ be diagonal. Thus, Quadratic Discriminant Analysis is more flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.R.R\n",
    "\n",
    "In ch4.R, line 13 is \"attach(Smarket).\" If that line was omitted from the script, which of the following lines would cause an error?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Libraries have been loaded!\"\n"
     ]
    }
   ],
   "source": [
    "LoadLibraries = function() {\n",
    "    library(MASS)\n",
    "    install.packages(\"ISLR\")\n",
    "    library(ISLR)\n",
    "    print(\"Libraries have been loaded!\")\n",
    "}\n",
    "\n",
    "LoadLibraries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Year'</li>\n",
       "\t<li>'Lag1'</li>\n",
       "\t<li>'Lag2'</li>\n",
       "\t<li>'Lag3'</li>\n",
       "\t<li>'Lag4'</li>\n",
       "\t<li>'Lag5'</li>\n",
       "\t<li>'Volume'</li>\n",
       "\t<li>'Today'</li>\n",
       "\t<li>'Direction'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Year'\n",
       "\\item 'Lag1'\n",
       "\\item 'Lag2'\n",
       "\\item 'Lag3'\n",
       "\\item 'Lag4'\n",
       "\\item 'Lag5'\n",
       "\\item 'Volume'\n",
       "\\item 'Today'\n",
       "\\item 'Direction'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Year'\n",
       "2. 'Lag1'\n",
       "3. 'Lag2'\n",
       "4. 'Lag3'\n",
       "5. 'Lag4'\n",
       "6. 'Lag5'\n",
       "7. 'Volume'\n",
       "8. 'Today'\n",
       "9. 'Direction'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n",
       "[7] \"Volume\"    \"Today\"     \"Direction\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in mean(glm.pred == Direction): object 'glm.pred' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in mean(glm.pred == Direction): object 'glm.pred' not found\nTraceback:\n",
      "1. mean(glm.pred == Direction)"
     ]
    }
   ],
   "source": [
    "mean(glm.pred==Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'train' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'train' not found\nTraceback:\n",
      "1. glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, \n .     family = binomial, subset = train)",
      "2. eval(mf, parent.frame())",
      "3. eval(expr, envir, enclos)",
      "4. stats::model.frame(formula = Direction ~ Lag1 + Lag2 + Lag3 + \n .     Lag4 + Lag5 + Volume, data = Smarket, subset = train, drop.unused.levels = TRUE)",
      "5. model.frame.default(formula = Direction ~ Lag1 + Lag2 + Lag3 + \n .     Lag4 + Lag5 + Volume, data = Smarket, subset = train, drop.unused.levels = TRUE)",
      "6. eval(substitute(subset), data, env)",
      "7. eval(expr, envir, enclos)"
     ]
    }
   ],
   "source": [
    "glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial, subset=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.Q.1\n",
    "\n",
    "Which of the following tools would be well suited for predicting if a student will get an A in a class based on the student's height, and parents’ income? Select all that apply:\n",
    "\n",
    "1. Linear Discriminant Analysis\n",
    "2. Linear Regression\n",
    "3. Logistic Regression\n",
    "4. Random Guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 1, 3.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:red\">\n",
    "1, 2, 3.\n",
    "</span>\n",
    "\n",
    "Whether or not a student gets an A is a categorical variables. Thus, we should use a classification technique like LDA or Logistic Regression. For binary classification, linear regression and LDA are almost equivalent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
