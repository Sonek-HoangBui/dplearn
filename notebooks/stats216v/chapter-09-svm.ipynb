{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px;\"><b>Stats216v: Statistical Learning</b></div>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center\">Stanford University</div>\n",
    "<div style=\"text-align: center\">Summer 2017</div>\n",
    "<div style=\"text-align: center\">Gyu-Ho Lee (<a href=\"mailto:gyuhox@gmail.com\">gyuhox@gmail.com</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.1.R1\n",
    "\n",
    "If $\\beta$ is not a unit vector but instead has length 2, then $\\sum_{j=1}^p \\beta_j X_j$ is\n",
    "\n",
    "1. twice the signed Euclidean distance from the separating hyperplane $\\sum_{j=1}^p \\beta_j X_j = 0$\n",
    "2. half the signed Euclidean distance from X to the separating hyperplane\n",
    "3. exactly the signed Euclidean distance from the separating hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 1. It is handy to assume that $\\beta$ is a unit vector, and geometrically it is the normal vector to the hyperplane.\n",
    "</span>\n",
    "\n",
    "We know $\\beta' = \\frac{1}{2}\\beta$ has length 1, so it is a unit vector in the same direction as $\\beta$. Therefore, $\\sum_{j=1}^p \\beta_j X_j = 2\\sum_{j=1}^p \\beta'_j X_j$, where $\\sum_{j=1}^p \\beta'_j X_j$ is the Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.2.R1\n",
    "\n",
    "If we increase C (the error budget) in an SVM, do you expect the standard error of $\\beta$ to increase or decrease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: C is larger, then the margin is wider and we allow more violations to it. So higher bias but lower variance. The standard deviation of the mean is the square root of the variance (the average squared deviation from the mean). The standard error of the mean is the expected value of the standard deviation of means of several samples. Therefore C is larger, then lower variance, then standard error of $\\beta$ **decreases**.\n",
    "</span>\n",
    "\n",
    "Increasing C makes the margin \"softer,\" so that the orientation of the separating hyperplane is influenced by more points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 9.3.R1\n",
    "\n",
    "True or False: If no linear boundary can perfectly classify all the training data, this means we need to use a feature expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: False.\n",
    "</span>\n",
    "\n",
    "As in any statistical problem, we will always do better on the training data if we use a feature expansion, but that doesn't mean we will improve the test error. Not all regression lines should perfectly interpolate all the training points, and not all classifiers should perfectly classify all the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 9.3.R2\n",
    "\n",
    "True or False: The computational effort required to solve a kernel support vector machine becomes greater and greater as the dimension of the basis increases. (Note: the dimension of the basis is not the same as p, the dimension of the predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Gyu-Ho's Answer: False.</span>\n",
    "\n",
    "The beauty of the \"kernel trick\" is that, even if there is an infinite-dimensional basis, we need only look at the n^2 inner products between training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 9.4.R1\n",
    "\n",
    "Recall that we obtain the ROC curve by classifying test points based on whether $\\hat f(x) > t$, and varying $t$.\n",
    "\n",
    "How large is the AUC (area under the ROC curve) for a classifier based on a completely random function $\\hat f(x)$ (that is, one for which the orderings of the $\\hat f(x_i)$ are completely random)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:red\">\n",
    "0.5.\n",
    "</span>\n",
    "\n",
    "If $\\hat f(x)$ is completely random, then $\\hat f(x_i)$ (and therefore the prediction for $y_i$) has nothing to do with $y_i$. Thus, the true positive rate and the false positive rate are both equal to the overall positive rate, and the ROC curve hugs the 45-degree line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.R.1\n",
    "\n",
    "Use svm in the e1071 package with the default settings (the default kernel is a radial kernel). What is the expected test error rate of this method (to within 10%)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.Q.1\n",
    "\n",
    "Suppose that after our computer works for an hour to fit an SVM on a large data set, we notice that $x_4$, the feature vector for the fourth example, was recorded incorrectly (say, one of the decimal points is obviously in the wrong place).\n",
    "\n",
    "However, your co-worker notices that the pair $(x_4, y_4)$ did not turn out to be a support point in the original fit. He says there is no need to re-fit the SVM on the corrected data set, because changing the value of a non-support point can't possibly change the fit.\n",
    "\n",
    "Is your co-worker correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: No.\n",
    "</span>\n",
    "\n",
    "When we change $x_4$, the fourth example might become a support point; if so, the fit may change. However, we could check whether $x_4$,$y_4$ is still not a support point even after correcting the value. If so, then we really don't need to re-fit the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
