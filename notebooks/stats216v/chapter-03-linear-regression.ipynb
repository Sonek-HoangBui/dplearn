{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px;\"><b>Stats216v: Statistical Learning</b></div>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center\">Stanford University</div>\n",
    "<div style=\"text-align: center\">Summer 2017</div>\n",
    "<div style=\"text-align: center\">Gyu-Ho Lee (<a href=\"mailto:gyuhox@gmail.com\">gyuhox@gmail.com</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.R1\n",
    "\n",
    "Why is linear regression important to understand? Select all that apply:\n",
    "\n",
    "1. The linear model is often correct.\n",
    "2. Linear regression is very extensible and can be used to capture nonlinear effects.\n",
    "3. Simple methods can outperform more complex ones if the data are noisy.\n",
    "4. Understanding simpler methods sheds light on more complex ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer:\n",
    "1, 3, 4.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:red\">2, 3, 4.</span>\n",
    "\n",
    "The linear model (and every other model) is hardly ever true, but it is an important piece in many more complex methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.R2\n",
    "\n",
    "Which of the following are true statements? Select all that apply:\n",
    "\n",
    "1. A 95% confidence interval is a random interval that contains the true parameter 95% of the time.\n",
    "2. The true parameter is a random value that has 95% chance of falling in the 95% confidence interval.\n",
    "3. I perform a linear regression and get a 95% confidence interval from 0.4 to 0.5. There is a 95% probability that the true parameter is between 0.4 and 0.5.\n",
    "4. The true parameter (unknown to me) is 0.5. If I sample data and construct a 95% confidence interval, the interval will contain 0.5 95% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer:\n",
    "1, 3.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:red\">1, 4.</span>\n",
    "\n",
    "Confidence intervals are a \"frequentist\" concept: the interval, and not the true parameter, is considered random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 3.2.R1\n",
    "\n",
    "We run a linear regression and the slope estimate is 0.5 with estimated standard error of 0.2. What is the largest value of $b$ for which we would NOT reject the null hypothesis that $β_{1} = b$? (assume normal approximation to $t$ distribution, and that we are using the 5% significance level for a two-sided test; need two significant digits of accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 0.892.\n",
    "</span>\n",
    "\n",
    "The 95% confidence interval $\\hat{β}_{1} ± 1.96 * S.E.(\\hat{β}_{1}) = 0.5 + 1.96 * 0.2 = 0.892$ contains all parameter values that would not be rejected at a 5% significance level. The critical value for a 95% confidence interval is 1.96."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 3.2.R2\n",
    "\n",
    "Which of the following indicates a fairly strong relationship between $X$ and $Y$?\n",
    "\n",
    "1. $R^{2} = 0.9$.\n",
    "2. The p-value for the null hypothesis $β_{1} = 0$ is 0.0001.\n",
    "3. The $t$-statistic for the null hypothesis $β_{1} = 0$ is 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Gyu-Ho's Answer: 1.</span>\n",
    "\n",
    "The $R^{2}$ is the correlation between the two variables and measures how closely they are associated. The $p$-value and $t$-statistic merely measure how strong is the evidence that there is a nonzero association. Even a weak effect can be extremely significant given enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 3.3.R1\n",
    "\n",
    "Suppose we are interested in learning about a relationship between $X_{1}$ and $Y$, which we would ideally like to interpret as causal.\n",
    "\n",
    "True or False: The estimate $\\hat{β}_{1}$ in a linear regression that controls for many variables (that is, a regression with many predictors in addition to $X_{1}$) is usually a more reliable measure of a causal relationship than $\\hat{β}_{1}$ from a univariate regression on $X_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer:\n",
    "False.\n",
    "</span>\n",
    "\n",
    "Adding lots of extra predictors to the model can just as easily muddy the interpretation of $\\hat{β}_{1}$ as it can clarify it. One often reads in media reports of academic studies that \"the investigators controlled for confounding variables,\" but be skeptical!\n",
    "\n",
    "Causal inference is a difficult and slippery topic, which cannot be answered with observational data alone without additional assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.R.R1\n",
    "\n",
    "What is the difference between $lm$($y$ ~ $x*z$) and $lm$($y$ ~ $I(x*z)$), when $x$ and $z$ are both numeric variables?\n",
    "\n",
    "1. The first one includes an interaction term between x and z, whereas the second uses the product of x and z as a predictor in the model.\n",
    "2. The second one includes an interaction term between x and z, whereas the first uses the product of x and z as a predictor in the model.\n",
    "3. The first includes only an interaction term for x and z, while the second includes both interaction effects and main effects.\n",
    "4. The second includes only an interaction term for x and z, while the first includes both interaction effects and main effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 4.\n",
    "</span>\n",
    "\n",
    "An interaction term between a numeric x and z is just the product of $x$ and $z$. The difference is that in the first model, lm processes the \"*\" operator between variables and automatically includes main effects, whereas in the latter model, the expression inside the $I()$ function is not parsed as a part of the formula, but rather is simply evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.R2\n",
    "\n",
    "What is the predicted balance for an African American? (within .01 accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: Flexible is better, when we have much data.\n",
    "</span>\n",
    "\n",
    "A flexible model will allow us to take full advantage of our large sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.Q\n",
    "\n",
    "Which of the following statements are true?\n",
    "\n",
    "<img src=\"./chapter-03-linear-regression-1.png\" alt=\"chapter-03-linear-regression-1.png\" style=\"width: 450px;\"/>\n",
    "<img src=\"./chapter-03-linear-regression-2.png\" alt=\"chapter-03-linear-regression-2.png\" style=\"width: 450px;\"/>\n",
    "\n",
    "1. In the balance vs. income * student model plotted on slide 44, the estimate of beta3 is negative.\n",
    "2. One advantage of using linear models is that the true regression function is often linear.\n",
    "3. If the F statistic is significant, all of the predictors have statistically significant effects.\n",
    "4. In a linear regression with several variables, a variable has a positive regression coefficient if and only if its correlation with the response is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Gyu-Ho's Answer: 4.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:red\">1.</span>\n",
    "\n",
    "We can see that the estimate of beta3 is negative because the slope of the student line is smaller than the slope of the non-student line. That is, being a student diminishes the effect of income on balance.\n",
    "\n",
    "The linear model is almost always wrong; however, it is often still useful.\n",
    "\n",
    "The F statistic tests the null hypothesis that none of the predictors has any effect. Rejecting that null means concluding that *some* predictor has an effect, not that *all* of them do.\n",
    "\n",
    "Positive correlation only means that the univariate regression has a positive correlation. See slide 20 for a counterexample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
