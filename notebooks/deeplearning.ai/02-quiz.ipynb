{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week1-Q1.\n",
    "\n",
    "What does the analogy “AI is the new electricity” refer to?\n",
    "\n",
    "1. Through the “smart grid”, AI is delivering a new wave of electricity.\n",
    "2. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before.\n",
    "3. Similar to electricity starting about 100 years ago, AI is transforming multiple industries.\n",
    "4. AI is powering personal devices in our homes and offices, similar to electricity.\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: 3.\n",
    "AI is transforming many fields from the car industry to agriculture to supply-chain.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week1-Q2.\n",
    "\n",
    "Which of these are reasons for Deep Learning recently taking off? (Check the three options that apply.)\n",
    "\n",
    "1. Deep learning has resulted in significant improvements in important applications such as online advertising, speech recognition, and image recognition.\n",
    "2. Neural Networks are a brand new field.\n",
    "3. We have access to a lot more data.\n",
    "4. We have access to a lot more computational power.\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: 1,3,4.\n",
    "The digitalization of our society has played a huge role in this. The development of hardware, perhaps especially GPU computing, has significantly improved deep learning algorithms' performance.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week1-Q3.\n",
    "\n",
    "Recall this diagram of iterating over different ML ideas. Which of the statements below are true? (Check all that apply.)\n",
    "\n",
    "<img src=\"images/cycle.png\" alt=\"cycle.png\" style=\"width:300px\"/>\n",
    "\n",
    "1. Being able to try out ideas quickly allows deep learning engineers to iterate more quickly.\n",
    "2. Faster computation can help speed up how long a team takes to iterate to a good idea.\n",
    "3. It is faster to train on a big dataset than a small dataset.\n",
    "4. Recent progress in deep learning algorithms has allowed us to train good models faster (even without changing the CPU/GPU hardware).\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: 1,2,4.\n",
    "For example, we discussed how switching from sigmoid to ReLU activation functions allows faster training.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### week1-Q4.\n",
    "\n",
    "When an experienced deep learning engineer works on a new problem, they can usually use insight from previous problems to train a good model on the first try, without needing to iterate multiple times through different models. True/False?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: False.\n",
    "Finding the characteristics of a model is key to have good performance. Although experience can help, it requires multiple iterations to build a good model.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week1-Q5.\n",
    "\n",
    "ReLU activation function?\n",
    "\n",
    "![relu-quiz.png](images/relu-quiz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week1-Q6.\n",
    "\n",
    "Images for cat recognition is an example of “structured” data, because it is represented as a structured array in a computer. True/False?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: False.\n",
    "Images for cat recognition is an example of “unstructured” data.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week1-Q7.\n",
    "\n",
    "A demographic dataset with statistics on different cities' population, GDP per capita, economic growth is an example of “unstructured” data because it contains data coming from different sources. True/False?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: False.\n",
    "A demographic dataset with statistics on different cities' population, GDP per capita, economic growth is an example of “structured” data by opposition to image, audio or text datasets.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week1-Q8.\n",
    "\n",
    "Why is an RNN (Recurrent Neural Network) used for machine translation, say translating English to French? (Check all that apply.)\n",
    "\n",
    "1. It can be trained as a supervised learning problem.\n",
    "2. It is strictly more powerful than a Convolutional Neural Network (CNN).\n",
    "3. It is applicable when the input/output is a sequence (e.g., a sequence of words).\n",
    "4. RNNs represent the recurrent process of Idea->Code->Experiment->Idea->....\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: 1,3.\n",
    "An RNN can map from a sequence of english words to a sequence of french words.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week1-Q9.\n",
    "\n",
    "In this diagram which we hand-drew in lecture, what do the horizontal axis (x-axis) and vertical axis (y-axis) represent?\n",
    "\n",
    "<img src=\"images/networks.png\" alt=\"networks.png\" style=\"width:550px\"/>\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: x-axis is the amount of data.\n",
    "y-axis (vertical axis) is the performance of the algorithm.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week1-Q10.\n",
    "\n",
    "Assuming the trends described in the previous question's figure are accurate (and hoping you got the axis labels right), which of the following are true? (Check all that apply.)\n",
    "\n",
    "1. Decreasing the training set size generally does not hurt an algorithm’s performance, and it may help significantly.\n",
    "2. Increasing the size of a neural network generally does not hurt an algorithm’s performance, and it may help significantly.\n",
    "3. Increasing the training set size generally does not hurt an algorithm’s performance, and it may help significantly.\n",
    "4. Decreasing the size of a neural network generally does not hurt an algorithm’s performance, and it may help significantly.\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: 2,3.\n",
    "According to the trends in the figure above, big networks usually perform better than small networks.\n",
    "Bringing more data to a model is almost always beneficial.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week2-Q3.\n",
    "\n",
    "Suppose img is a `(32,32,3)` array, representing a 32x32 image with 3 color channels red, green and blue. How do you reshape this into a column vector?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: x = img.reshape((32\\*32\\*3,1)).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week2-Q4.\n",
    "\n",
    "Consider the two following random arrays \"a\" and \"b\".\n",
    "What will be the shape of \"c\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(2, 3) # a.shape = (2, 3)\n",
    "b = np.random.randn(2, 1) # b.shape = (2, 1)\n",
    "c = a + b\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Answer: This is broadcasting. b (column vector) is copied 3 times so that it can be summed to each column of a.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week2-Q5.\n",
    "\n",
    "Consider the two following random arrays \"a\" and \"b\".\n",
    "What will be the shape of \"c\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(4, 3) # a.shape = (4, 3)\n",
    "b = np.random.randn(3, 2) # b.shape = (3, 2)\n",
    "\n",
    "# operands could not be broadcast together with shapes (4,3) (3,2) \n",
    "# print((a*b).shape)\n",
    "\n",
    "print((np.dot(a,b)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Answer: The computation cannot happen because the sizes don't match. It's going to be \"Error\"!\n",
    "In numpy the \"*\" operator indicates element-wise multiplication. It is different from \"np.dot()\". If you would try \"c = np.dot(a,b)\" you would get c.shape = (4, 2).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week2-Q7.\n",
    "\n",
    "Recall that \"np.dot(a,b)\" performs a matrix multiplication on a and b, whereas \"a*b\" performs an element-wise multiplication.\n",
    "\n",
    "Consider the two following random arrays \"a\" and \"b\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 45)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(12288, 150) # a.shape = (12288, 150)\n",
    "b = np.random.randn(150, 45) # b.shape = (150, 45)\n",
    "c = np.dot(a,b)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Answer: Remember that a np.dot(a, b) has shape (number of rows of a, number of columns of b). The sizes match because : \"number of columns of a = 150 = number of rows of b\"\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week2-Q8.\n",
    "\n",
    "Consider the following code snippet. How do you vectorize this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3,4)\n",
    "b = np.random.randn(4,1)\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        c[i][j] = a[i][j] + b[j]\n",
    "\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "c = a + b.T\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week2-Q9.\n",
    "\n",
    "Consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3, 3)\n",
    "b = np.random.randn(3, 1)\n",
    "c = a*b\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will be c? (If you’re not sure, feel free to run this in python to find out).\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: This will invoke broadcasting, so b is copied three times to become (3,3), and ∗ is an element-wise product so c.shape will be (3, 3).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week3-Q5.\n",
    "\n",
    "Consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.random.randn(4,3)\n",
    "B = np.sum(A, axis=1, keepdims=True)\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Answer: We use (keepdims = True) to make sure that A.shape is (4,1) and not (4, ). It makes our code more rigorous.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week2-Q10.\n",
    "\n",
    "Consider the following computation graph.\n",
    "\n",
    "<img src=\"images/computation.png\" alt=\"computation.png\" style=\"width:600px\"/>\n",
    "\n",
    "What is the output J?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: `J = (a - 1) * (b + c)`\n",
    "\n",
    "`J = u + v - w = a*b + a*c - (b + c) = a * (b + c) - (b + c) = (a - 1) * (b + c)`.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week2-Q2.\n",
    "\n",
    "Which of these is the \"Logistic Loss\"?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: $\\mathcal{L}^{(i)}(\\hat{y}^{(i)}, y^{(i)}) = y^{(i)}\\log(\\hat{y}^{(i)}) + (1- y^{(i)})\\log(1-\\hat{y}^{(i)})$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week-Q6.\n",
    "\n",
    "How many layers does this network have?\n",
    "\n",
    "<img src=\"images/n4.png\" alt=\"n4.png\" style=\"width:400px\" />\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: The number of layers $L$ is 4. The number of hidden layers is 3. As seen in lecture, the number of layers is counted as the number of hidden layers + 1. The input and output layers are not counted as hidden layers.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week3-Q1.\n",
    "\n",
    "Which of the following are true? (Check all that apply.)\n",
    "\n",
    "- $a^{[2]}$ denotes the activation vector of the $2^{nd}$ layer. <span style=\"color:blue\">(True)</span>\n",
    "- $a^{[2]}_4$ is the activation output by the $4^{th}$ neuron of the $2^{nd}$ layer. <span style=\"color:blue\">(True)</span>\n",
    "- $a^{[2](12)}$ denotes the activation vector of the $2^{nd}$ layer for the $12^{th}$ training example.<span style=\"color:blue\">(True)</span>\n",
    "- $X$ is a matrix in which each column is one training example. <span style=\"color:blue\">(True)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week2-Q1.\n",
    "\n",
    "What does a neuron compute?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: A neuron computes a linear function `(z = Wx + b)` followed by an activation function.\n",
    "The output of a neuron is `a = g(Wx + b)` where `g` is the activation function (sigmoid, tanh, ReLU, ...).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week3-Q3.\n",
    "\n",
    "Vectorized implementation of forward propagation for layer $l$, where $1 \\leq l \\leq L$?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer:\n",
    "$$Z^{[l]} = W^{[l]} A^{[l-1]}+ b^{[l]}$$\n",
    "$$A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week-Q4.\n",
    "\n",
    "Vectorization allows you to compute forward propagation in an L-layer neural network without an explicit for-loop (or any other explicit iterative loop) over the layers l=1, 2, …,L. True/False?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: False.\n",
    "Forward propagation propagates the input through the layers, although for shallow networks we may just write all the lines ($a^{[2]} = g^{[2]}(z^{[2]})$, $z^{[2]}= W^{[2]}a^{[1]}+b^{[2]}$, ...) in a deeper network, we cannot avoid a for loop iterating over the layers: ($a^{[l]} = g^{[l]}(z^{[l]})$, $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$, ...).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week-Q5.\n",
    "\n",
    "Assume we store the values for $n^{[l]}$ in an array called layers, as follows: layer_dims = $[n_x,4,3,2,1]$. So layer 1 has four hidden units, layer 2 has 3 hidden units and so on. Which of the following for-loops will allow you to initialize the parameters for the model?\n",
    "\n",
    "```python\n",
    "for(i in range(1, len(layer_dims))):\n",
    "    parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1]) * 0.01\n",
    "    parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week4-Q10.\n",
    "\n",
    "Whereas the previous question used a specific network, in the general case what is the dimension of $W^{[l]}$, the weight matrix associated with layer $l$?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: $W^{[l]}$ has shape $(n^{[l]}, n^{[l-1]})$.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week4-Q9.\n",
    "\n",
    "Which of the following statements are True? (Check all that apply).\n",
    "\n",
    "<img src=\"./images/n2.png\" alt=\"n2.png\" style=\"width: 450px;\"/>\n",
    "\n",
    "1. $W^{[1]}$ will have shape (4, 4). <span style=\"color:blue\">True, shape of $W^{[l]}$ is $(n^{[l]}, n^{[l-1]})$</span>\n",
    "3. $W^{[2]}$ will have shape (3, 4). <span style=\"color:blue\">True, shape of $W^{[l]}$ is $(n^{[l]}, n^{[l-1]})$</span>\n",
    "3. $W^{[3]}$ will have shape (1, 3). <span style=\"color:blue\">True, shape of $W^{[l]}$ is $(n^{[l]}, n^{[l-1]})$</span>\n",
    "2. $b^{[1]}$ will have shape (4, 1). <span style=\"color:blue\">True, shape of $b^{[l]}$ is $(n^{[l]}, 1)$</span>\n",
    "4. $b^{[2]}$ will have shape (3, 1). <span style=\"color:blue\">True, shape of $b^{[l]}$ is $(n^{[l]}, 1)$</span>\n",
    "4. $b^{[3]}$ will have shape (1, 1). <span style=\"color:blue\">True, shape of $b^{[l]}$ is $(n^{[l]}, 1)$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week3-Q9.\n",
    "\n",
    "Consider the following 1 hidden layer neural network:\n",
    "\n",
    "![1-hidden.png](images/1-hidden.png)\n",
    "\n",
    "Which of the following statements are True? (Check all that apply).\n",
    "\n",
    "- $W^{[1]}$ will have shape (4, 2) <span style=\"color:blue\">(True)</span>\n",
    "- $W^{[2]}$ will have shape (1, 4) <span style=\"color:blue\">(True)</span>\n",
    "- $b^{[1]}$ will have shape (4, 1) <span style=\"color:blue\">(True)</span>\n",
    "- $b^{[2]}$ will have shape (1, 1) <span style=\"color:blue\">(True)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week2-Q6.\n",
    "\n",
    "Suppose you have nx input features per example. Recall that $X = [x^{(1)} x^{(2)} ... x^{(m)}]$. What is the dimension of X?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: $(n_x, m)$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week3-Q10.\n",
    "\n",
    "In the same network as the previous question, what are the dimensions of $Z^{[1]}$ and $A^{[1]}$?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: $Z^{[1]}$ and $A^{[1]}$ are (4, m).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week3-Q4.\n",
    "\n",
    "You are building a binary classifier for recognizing cucumbers (y=1) vs. watermelons (y=0). Which one of these activation functions would you recommend using for the output layer?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: sigmoid.\n",
    "Sigmoid outputs a value between 0 and 1 which makes it a very good choice for binary classification. You can classify as 0 if the output is less than 0.5 and classify as 1 if the output is more than 0.5. It can be done with tanh as well but it is less convenient as the output is between -1 and 1.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week3-Q2.\n",
    "\n",
    "The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer. True/False?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: True, as seen in lecture the output of the tanh is between -1 and 1, it thus centers the data which makes the learning simpler for the next layer.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week3-Q8.\n",
    "\n",
    "You have built a network using the tanh activation for all the hidden units. You initialize the weights to relative large values, using `np.random.randn(..,..)*1000`. What will happen?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: This will cause the inputs of the tanh to also be very large, thus causing gradients to be close to zero. The optimization algorithm will thus become slow. `tanh` becomes flat for large values, this leads its gradient to be close to zero. This slows down the optimization algorithm.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week3-Q6.\n",
    "\n",
    "Suppose you have built a neural network. You decide to initialize the weights and biases to be zero. Which of the following statements is true?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: Each neuron in the first hidden layer will perform the same computation. So even after multiple iterations of gradient descent each neuron in the layer will be computing the same thing as other neurons.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week3-Q7.\n",
    "\n",
    "Logistic regression’s weights w should be initialized randomly rather than to all zeros, because if you initialize to all zeros, then logistic regression will fail to learn a useful decision boundary because it will fail to “break symmetry”, True/False?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: False.\n",
    "Logistic Regression doesn't have a hidden layer. If you initialize the weights to zeros, the first example x fed in the logistic regression will output zero but the derivatives of the Logistic Regression depend on the input x (because there's no hidden layer) which is not zero. So at the second iteration, the weights values follow x's distribution and are different from each other if x is not a constant vector.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### week4-Q1.\n",
    "\n",
    "What is the \"cache\" used for in our implementation of forward propagation and backward propagation?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives. The \"cache\" records values from the forward propagation units and sends it to the backward propagation units because it is needed to compute the chain rule derivatives.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week4-Q2.\n",
    "\n",
    "Among the following, which ones are \"hyperparameters\"? (Check all that apply.)\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: learning rate $\\alpha$, number of layers $L$ in the neural network, number of iterations, size of the hidden layers $n^{[l]}$.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Not hyperparameters: bias vectors $b^{[l]}$, weight matrices $W^{[l]}$, activation values $a^{[l]}$.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week4-Q3.\n",
    "\n",
    "Which of the following statements is true?\n",
    "\n",
    "1. The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.\n",
    "2. The earlier layers of a neural network are typically computing more complex features of the input than the deeper layers.\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: 1.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week-Q7.\n",
    "\n",
    "During forward propagation, in the forward function for a layer l you need to know what is the activation function in a layer (Sigmoid, tanh, ReLU, etc.). During backpropagation, the corresponding backward function also needs to know what is the activation function for layer l, since the gradient depends on it. True/False?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: True, as you've seen in the week 3 each activation has a different derivative. Thus, during backpropagation you need to know which activation was used in the forward propagation to be able to compute the correct derivative.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### week-Q8.\n",
    "\n",
    "There are certain functions with the following properties:\n",
    "\n",
    "(i) To compute the function using a shallow network circuit, you will need a large network (where we measure size by the number of logic gates in the network), but (ii) To compute it using a deep network circuit, you need only an exponentially smaller network. True/False?\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "Answer: True.\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
